{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "73249e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from DSN import DeepSpectralNet\n",
    "from Tool import split_dataset,standardize_data\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a49c9d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import root_mean_squared_error,r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3bf9a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_slice_localization():\n",
    "    \"\"\"\n",
    "    UCI Slice Localization\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. URL Directe du fichier ZIP sur le site de l'Université de Californie (UCI)\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00206/slice_localization_data.zip\"\n",
    "\n",
    "    print(\"Téléchargement et lecture du fichier CSV (cela peut prendre quelques secondes)...\")\n",
    "    df = pd.read_csv(url, compression='zip')\n",
    "\n",
    "    # 2. Nettoyage et Séparation\n",
    "    # La colonne 'reference' est la cible (la position de la slice)\n",
    "    # La colonne 'patientId' est un identifiant qu'on retire généralement pour l'apprentissage\n",
    "    y = df['reference'].values\n",
    "    X = df.drop(columns=['reference', 'patientId']).values\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Shape de X : {X.shape}\") \n",
    "    print(f\"Shape de y : {y.shape}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    if X.shape == (53500, 384):\n",
    "        print(\"✅ SUCCÈS : C'est le bon dataset !\")\n",
    "    else:\n",
    "        print(\"❌ ERREUR : Toujours pas le bon format.\")\n",
    "\n",
    "    return X, y\n",
    "\n",
    "class RegressionDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1bfd7aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement et lecture du fichier CSV (cela peut prendre quelques secondes)...\n",
      "------------------------------\n",
      "Shape de X : (53500, 384)\n",
      "Shape de y : (53500,)\n",
      "------------------------------\n",
      "✅ SUCCÈS : C'est le bon dataset !\n",
      "Train: 37450\n",
      "Val: 8025\n",
      "Test: 8025\n",
      "Input dim: torch.Size([384])\n"
     ]
    }
   ],
   "source": [
    "# Load\n",
    "X, y = load_slice_localization()\n",
    "\n",
    "# Split\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_dataset(X, y)\n",
    "\n",
    "# Standardize\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, scaler_X, scaler_y = standardize_data(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    X_test, y_test\n",
    ")\n",
    "\n",
    "# PyTorch datasets\n",
    "train_dataset = RegressionDataset(X_train, y_train)\n",
    "val_dataset   = RegressionDataset(X_val, y_val)\n",
    "test_dataset  = RegressionDataset(X_test, y_test)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "print(\"Train:\", len(train_dataset))\n",
    "print(\"Val:\", len(val_dataset))\n",
    "print(\"Test:\", len(test_dataset))\n",
    "print(\"Input dim:\", train_dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53db0154",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=(256, 128, 64),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=300,\n",
    "    early_stopping=True,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142dcc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0437443904016065\n",
      "R²: 0.9980656175783365\n",
      "139777\n"
     ]
    }
   ],
   "source": [
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_test)\n",
    "mse = root_mean_squared_error(y_test, y_pred)\n",
    "print(\"RMSE:\", mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R²:\", r2)\n",
    "\n",
    "def count_params_mlp(model):\n",
    "    return sum(w.size + b.size for w, b in zip(model.coefs_, model.intercepts_))\n",
    "print(count_params_mlp(mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "83e5f217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sarcos(path_train, path_test=None, standardize=True):\n",
    "    \"\"\"\n",
    "    Charge le dataset SARCOS inverse dynamics.\n",
    "    \n",
    "    Inputs:\n",
    "        - X : 21 features (positions, vitesses, accélérations)\n",
    "        - y : 7 torques\n",
    "    \n",
    "    Returns:\n",
    "        X_train, y_train, X_test, y_test (si test fourni)\n",
    "    \"\"\"\n",
    "    # -------- TRAIN --------\n",
    "    train_data = loadmat(path_train)\n",
    "    train = train_data['sarcos_inv']\n",
    "\n",
    "    X_train = train[:, :21]\n",
    "    y_train = train[:, 21:]\n",
    "\n",
    "    # -------- TEST --------\n",
    "    if path_test is not None:\n",
    "        test_data = loadmat(path_test)\n",
    "        test = test_data['sarcos_inv_test']\n",
    "\n",
    "        X_test = test[:, :21]\n",
    "        y_test = test[:, 21:]\n",
    "    else:\n",
    "        X_test, y_test = None, None\n",
    "\n",
    "    # -------- STANDARDIZATION --------\n",
    "    if standardize:\n",
    "        scaler_X = StandardScaler()\n",
    "        scaler_y = StandardScaler()\n",
    "\n",
    "        X_train = scaler_X.fit_transform(X_train)\n",
    "        y_train = scaler_y.fit_transform(y_train)\n",
    "\n",
    "        if X_test is not None:\n",
    "            X_test = scaler_X.transform(X_test)\n",
    "            y_test = scaler_y.transform(y_test)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e835fe87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44484, 21)\n",
      "(44484, 7)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = load_sarcos(\n",
    "    \"data/sarcos_inv.mat\",\n",
    "    \"data/sarcos_inv_test.mat\"\n",
    ")\n",
    "\n",
    "print(X_train.shape)  # (~44k, 21)\n",
    "print(y_train.shape)  # (~44k, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e9f0140a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.09432834536511595\n",
      "R²: 0.9906200489375327\n",
      "47239\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=(256, 128, 64),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=300,\n",
    "    early_stopping=True,\n",
    "    random_state=42\n",
    ")\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_test)\n",
    "mse = root_mean_squared_error(y_test, y_pred)\n",
    "print(\"RMSE:\", mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R²:\", r2)\n",
    "\n",
    "def count_params_mlp(model):\n",
    "    return sum(w.size + b.size for w, b in zip(model.coefs_, model.intercepts_))\n",
    "print(count_params_mlp(mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c7b8c347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 40000, Test size (The Trap): 10000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dataset Feynman I.43.16 (Viscosité / Cinétique des gaz)\n",
    "# Formule : y = mu * u * n / (v - u)  (Avec une singularité quand v approche u)\n",
    "# C'est MORTEL pour un MLP à cause de la division et de l'asymptote.\n",
    "\n",
    "def feynman_function(X):\n",
    "    # X columns: mu, u, n, v\n",
    "    mu = X[:, 0]\n",
    "    u  = X[:, 1]\n",
    "    n  = X[:, 2]\n",
    "    v  = X[:, 3] \n",
    "    # On s'assure que v > u pour éviter la division par zéro directe, mais on s'en approche\n",
    "    return mu * u * n / (v - u + 1e-6)\n",
    "\n",
    "N = 50000\n",
    "X = np.random.rand(N, 4) * 10  # Valeurs entre 0 et 10\n",
    "# On force v à être proche de u pour créer des pics\n",
    "X[:, 3] = X[:, 1] + np.random.rand(N) * 2 \n",
    "\n",
    "y = feynman_function(X)\n",
    "\n",
    "mask_train = (y < np.percentile(y, 80)) # On apprend sur les valeurs \"calmes\"\n",
    "X_train, y_train = X[mask_train], y[mask_train]\n",
    "X_test, y_test   = X[~mask_train], y[~mask_train] # On teste sur les pics extrêmes\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Test size (The Trap): {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fbd43c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 119421.92440175549\n",
      "R²: -0.0005458077095688019\n",
      "198913\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=(256,256,256,256),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=300,\n",
    "    early_stopping=True,\n",
    "    random_state=42\n",
    ")\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_test)\n",
    "mse = root_mean_squared_error(y_test, y_pred)\n",
    "print(\"RMSE:\", mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R²:\", r2)\n",
    "\n",
    "def count_params_mlp(model):\n",
    "    return sum(w.size + b.size for w, b in zip(model.coefs_, model.intercepts_))\n",
    "print(count_params_mlp(mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dd3025ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de paramètres : 52477\n",
      "Paramètres de la première couche : 60\n"
     ]
    }
   ],
   "source": [
    "d_input = X_train.shape[1]\n",
    "epochs = 50\n",
    "learning_rate = 1e-3\n",
    "n_samples = X.shape[0]\n",
    "batch_size = 128\n",
    "\n",
    "dims = [d_input,8,128,128,8,1]\n",
    "dsn = DeepSpectralNet(dims, ortho_mode=None,use_layernorm=True)\n",
    "\n",
    "# 1. Obtenir le nombre total de paramètres\n",
    "total_params = dsn.num_parameters\n",
    "print(f\"Nombre total de paramètres : {total_params}\")\n",
    "\n",
    "# 2. Vérifier une couche spécifique\n",
    "layer_params = dsn.layers[0].num_parameters\n",
    "print(f\"Paramètres de la première couche : {layer_params}\")\n",
    "\n",
    "# ==== Optimiseur et loss ====\n",
    "optimizer = torch.optim.AdamW(dsn.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.1, patience=100\n",
    ")\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "84413939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40000, 4])\n",
      "torch.Size([40000, 1])\n"
     ]
    }
   ],
   "source": [
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_t  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "print(X_train_t.shape)  # (N_train, d_input)\n",
    "print(y_train_t.shape)  # (N_train, 1)\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_t, y_test_t)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.AdamW(dsn.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.1, patience=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "11c89221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300 | Train Loss: 12165.911311 | Val Loss: 14268249930.700399 | LR: 0.001000\n",
      "Epoch 21/300 | Train Loss: 469.842741 | Val Loss: 14266655398.231600 | LR: 0.001000\n",
      "Epoch 41/300 | Train Loss: 224.623443 | Val Loss: 14266220610.919600 | LR: 0.001000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m dsn\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      8\u001b[0m total_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m Xb, yb \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     12\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m dsn(Xb)         \n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataset.py:209\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataset.py:209\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # ======== Training ========\n",
    "    dsn.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    for Xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = dsn(Xb)         \n",
    "        loss = criterion(y_pred, yb)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(dsn.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item() * Xb.size(0)\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "    train_loss_history.append(avg_train_loss)\n",
    "    #scheduler.step(avg_train_loss)\n",
    "\n",
    "    # ======== Validation ========\n",
    "    dsn.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in test_loader:  \n",
    "            y_pred = dsn(Xb)\n",
    "            loss = criterion(y_pred, yb)\n",
    "            total_val_loss += loss.item() * Xb.size(0)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(test_loader.dataset)\n",
    "    val_loss_history.append(avg_val_loss)\n",
    "\n",
    "    # ======== Logging ========\n",
    "    if epoch % 20 == 0 or epoch == 0:\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.6f} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.6f} | LR: {current_lr:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b7d5612c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 119441.32276028839\n",
      "R²: -0.0008708824017511496\n",
      "198913\n"
     ]
    }
   ],
   "source": [
    "dsn.eval()\n",
    "with torch.no_grad(): y_pred = dsn(X_test_t) \n",
    "\n",
    "mse = root_mean_squared_error(y_test, y_pred)\n",
    "print(\"RMSE:\", mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R²:\", r2)\n",
    "\n",
    "def count_params_mlp(model):\n",
    "    return sum(w.size + b.size for w, b in zip(model.coefs_, model.intercepts_))\n",
    "print(count_params_mlp(mlp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
