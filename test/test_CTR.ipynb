{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f45cd7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Chargement des données...\n",
      "Users: 6038, Movies: 3533\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import roc_auc_score # Ajout de la métrique AUC\n",
    "\n",
    "# On suppose que DSN.py est dans le même dossier\n",
    "# On va utiliser DeepMultiBasisNet qui est souvent plus performant que le simple DeepSpectralNet\n",
    "from DSN import DeepMultiBasisNet\n",
    "from SBN import DeepMultiBasisBilinearNet\n",
    "\n",
    "# --- 1. SETUP & SEED ---\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                 else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# --- 2. DATA LOADING (Identique à ton code, propre) ---\n",
    "def load_movielens(path=\"data/ml-1m/ratings.dat\"):\n",
    "    # Si tu n'as pas le fichier localement, assure-toi que le path est bon\n",
    "    # Pour l'exemple, on peut créer un dummy dataframe si le fichier n'existe pas\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            path, sep=\"::\", engine=\"python\", \n",
    "            names=[\"user_id\", \"movie_id\", \"rating\", \"timestamp\"]\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        print(\"Fichier non trouvé, génération de données synthétiques pour le test...\")\n",
    "        df = pd.DataFrame({\n",
    "            \"user_id\": np.random.randint(0, 100, 10000),\n",
    "            \"movie_id\": np.random.randint(0, 50, 10000),\n",
    "            \"rating\": np.random.randint(1, 6, 10000),\n",
    "            \"timestamp\": range(10000)\n",
    "        })\n",
    "\n",
    "    df[\"label\"] = (df[\"rating\"] >= 4).astype(int)\n",
    "    df = df[df[\"label\"] == 1]\n",
    "    return df\n",
    "\n",
    "def temporal_split(df):\n",
    "    df = df.sort_values([\"user_id\", \"timestamp\"])\n",
    "    train, val, test = [], [], []\n",
    "    for u, g in df.groupby(\"user_id\"):\n",
    "        if len(g) < 3:\n",
    "            train.append(g)\n",
    "        else:\n",
    "            train.append(g.iloc[:-2])\n",
    "            val.append(g.iloc[-2:-1])\n",
    "            test.append(g.iloc[-1:])\n",
    "    return pd.concat(train), pd.concat(val), pd.concat(test)\n",
    "\n",
    "def reindex(train_df, val_df, test_df):\n",
    "    users = pd.concat([train_df, val_df, test_df])[\"user_id\"].unique()\n",
    "    movies = pd.concat([train_df, val_df, test_df])[\"movie_id\"].unique()\n",
    "    user_map = {u: i for i, u in enumerate(users)}\n",
    "    movie_map = {m: i for i, m in enumerate(movies)}\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        df[\"user_id\"] = df[\"user_id\"].map(user_map)\n",
    "        df[\"movie_id\"] = df[\"movie_id\"].map(movie_map)\n",
    "    return train_df, val_df, test_df, user_map, movie_map\n",
    "\n",
    "# --- 3. DATASETS ---\n",
    "class MovieLensTrainDataset(Dataset):\n",
    "    def __init__(self, df, num_movies):\n",
    "        self.users = torch.tensor(df.user_id.values, dtype=torch.long)\n",
    "        self.movies = torch.tensor(df.movie_id.values, dtype=torch.long)\n",
    "        self.positives = set(zip(df.user_id, df.movie_id))\n",
    "        self.num_movies = num_movies\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        u = self.users[idx]\n",
    "        pos_m = self.movies[idx]\n",
    "        while True:\n",
    "            neg_m = random.randint(0, self.num_movies - 1)\n",
    "            if (u.item(), neg_m) not in self.positives:\n",
    "                break\n",
    "        \n",
    "        # On retourne u, m, label séparément pour faciliter le batching\n",
    "        # Shape finale attendue par sample : (2,) pour user, (2,) pour item, (2,) pour label\n",
    "        return {\n",
    "            \"user_id\": torch.tensor([u, u]), \n",
    "            \"movie_id\": torch.tensor([pos_m, neg_m]),\n",
    "            \"label\": torch.tensor([1.0, 0.0])\n",
    "        }\n",
    "\n",
    "class MovieLensEvalDataset(Dataset):\n",
    "    def __init__(self, df, positives, num_movies, num_neg=99): # 99 negs est standard pour le ranking\n",
    "        self.samples = []\n",
    "        for _, row in df.iterrows():\n",
    "            u, pos_m = row.user_id, row.movie_id\n",
    "            negs = []\n",
    "            while len(negs) < num_neg:\n",
    "                m = random.randint(0, num_movies - 1)\n",
    "                if (u, m) not in positives:\n",
    "                    negs.append(m)\n",
    "            self.samples.append((u, pos_m, negs))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        u, pos_m, negs = self.samples[idx]\n",
    "        movies = [pos_m] + negs\n",
    "        labels = [1] + [0] * len(negs)\n",
    "        return {\n",
    "            \"user_id\": torch.tensor([u] * len(movies)),\n",
    "            \"movie_id\": torch.tensor(movies),\n",
    "            \"label\": torch.tensor(labels, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "# --- 4. PREPARATION DATA ---\n",
    "print(\"Chargement des données...\")\n",
    "df = load_movielens() \n",
    "train_df, val_df, test_df = temporal_split(df)\n",
    "train_df, val_df, test_df, user_map, movie_map = reindex(train_df, val_df, test_df)\n",
    "\n",
    "num_users = len(user_map)\n",
    "num_movies = len(movie_map)\n",
    "print(f\"Users: {num_users}, Movies: {num_movies}\")\n",
    "\n",
    "train_ds = MovieLensTrainDataset(train_df, num_movies)\n",
    "val_ds = MovieLensEvalDataset(val_df, set(zip(train_df.user_id, train_df.movie_id)), num_movies, num_neg=19) # 19 negs pour aller plus vite en validation\n",
    "test_ds = MovieLensEvalDataset(test_df, set(zip(train_df.user_id, train_df.movie_id)), num_movies, num_neg=99)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False) # Batch size 1 car longueur variable de negs possible (sinon padding)\n",
    "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "# --- 5. LE MODÈLE DSN COMPLET (AVEC EMBEDDINGS) ---\n",
    "class DSNRecModel(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embed_dim, layers_dim, ortho_mode='hard'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # A. Embeddings : Indispensables pour convertir les IDs en vecteurs\n",
    "        self.user_embedding = nn.Embedding(num_users, embed_dim)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embed_dim)\n",
    "        \n",
    "        # Initialisation correcte des embeddings\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.movie_embedding.weight)\n",
    "\n",
    "        # B. Backbone DSN\n",
    "        # L'entrée du DSN sera : Embed_User + Embed_Movie\n",
    "        in_dim = embed_dim * 2 \n",
    "        \n",
    "        # On insère la dimension d'entrée au début de la liste des layers\n",
    "        full_layers_dim = [in_dim] + layers_dim\n",
    "        \n",
    "        # On utilise DeepMultiBasisNet (plus puissant que DeepSpectralNet simple)\n",
    "        # Tu peux changer pour DeepSpectralNet si tu préfères\n",
    "        self.backbone = DeepMultiBasisBilinearNet(\n",
    "            layers_dim=full_layers_dim,\n",
    "            num_bases=8,             # Hyperparamètre DSN\n",
    "            ortho_mode=None,\n",
    "            use_final_linear=False,   # Important pour avoir des logits non bornés\n",
    "            use_layernorm=True,\n",
    "            use_residual=False\n",
    "        )\n",
    "\n",
    "    def forward(self, user_ids, movie_ids):\n",
    "        # 1. Lookups\n",
    "        u_emb = self.user_embedding(user_ids) # [Batch, Embed_Dim]\n",
    "        m_emb = self.movie_embedding(movie_ids) # [Batch, Embed_Dim]\n",
    "        \n",
    "        # 2. Concaténation (Feature Interaction input)\n",
    "        x = torch.cat([u_emb, m_emb], dim=-1) # [Batch, Embed_Dim * 2]\n",
    "        \n",
    "        # 3. DSN\n",
    "        logits = self.backbone(x) # [Batch, 1]\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# --- 7. BOUCLES D'ENTRAÎNEMENT CORRIGÉES ---\n",
    "\n",
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch in loader:\n",
    "        # batch['user_id'] shape : [Batch, 2] (pos, neg)\n",
    "        # On doit tout aplatir pour que le modèle voit une liste de samples [Batch * 2]\n",
    "        user_ids = batch[\"user_id\"].view(-1).to(device)\n",
    "        movie_ids = batch[\"movie_id\"].view(-1).to(device)\n",
    "        labels = batch[\"label\"].view(-1).float().to(device) # Shape [Batch * 2]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(user_ids, movie_ids).squeeze(-1) # Output [Batch*2, 1] -> [Batch*2]\n",
    "        \n",
    "        # Loss\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Ajout de la loss d'orthogonalité si mode 'soft' (optionnel ici car 'hard')\n",
    "        # loss += 0.1 * model.backbone.get_total_ortho_loss(torch.nn.functional.mse_loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "\n",
    "    return total_loss / (len(loader.dataset) * 2) # *2 car chaque sample a 1 pos et 1 neg\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in loader:\n",
    "        # Ici batch_size=1, mais chaque item contient [1, Num_Candidats]\n",
    "        user_ids = batch[\"user_id\"].view(-1).to(device)\n",
    "        movie_ids = batch[\"movie_id\"].view(-1).to(device)\n",
    "        labels = batch[\"label\"].view(-1).float().to(device)\n",
    "        \n",
    "        logits = model(user_ids, movie_ids).squeeze(-1)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        \n",
    "        all_preds.append(probs.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    # Concaténation globale pour calculer l'AUC global ou moyen\n",
    "    flat_preds = np.concatenate(all_preds)\n",
    "    flat_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    # Calcul des métriques\n",
    "    auc = roc_auc_score(flat_labels, flat_preds)\n",
    "    \n",
    "    # Précision simple (seuil 0.5)\n",
    "    acc = ((flat_preds > 0.5) == flat_labels).mean()\n",
    "    \n",
    "    return {\"auc\": auc, \"accuracy\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f8d27f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle créé avec 899785 paramètres.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 6. CONFIGURATION ---\n",
    "EMBED_DIM = 64\n",
    "HIDDEN_LAYERS = [16,8,1] # Le dernier doit être 1 pour la prédiction binaire\n",
    "\n",
    "model = DSNRecModel(\n",
    "    num_users=num_users,\n",
    "    num_movies=num_movies,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    layers_dim=HIDDEN_LAYERS,\n",
    "    ortho_mode=None\n",
    ").to(device)\n",
    "\n",
    "print(f\"Modèle créé avec {model.backbone.num_parameters + num_users*EMBED_DIM + num_movies*EMBED_DIM} paramètres.\")\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6a3577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "889e524e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 0.4558 | Val AUC: 0.8449 | Val Acc: 74.39%\n",
      "Epoch 02 | Train Loss: 0.4015 | Val AUC: 0.8703 | Val Acc: 79.50%\n",
      "Epoch 03 | Train Loss: 0.3659 | Val AUC: 0.8853 | Val Acc: 82.33%\n",
      "Epoch 04 | Train Loss: 0.3461 | Val AUC: 0.8920 | Val Acc: 83.83%\n",
      "Epoch 05 | Train Loss: 0.3316 | Val AUC: 0.8949 | Val Acc: 84.47%\n",
      "Epoch 06 | Train Loss: 0.3200 | Val AUC: 0.8971 | Val Acc: 84.15%\n",
      "Epoch 07 | Train Loss: 0.3089 | Val AUC: 0.8954 | Val Acc: 83.56%\n",
      "Epoch 08 | Train Loss: 0.2999 | Val AUC: 0.8976 | Val Acc: 82.68%\n",
      "Epoch 09 | Train Loss: 0.2914 | Val AUC: 0.8983 | Val Acc: 84.35%\n",
      "Epoch 10 | Train Loss: 0.2831 | Val AUC: 0.8988 | Val Acc: 84.79%\n",
      "Epoch 11 | Train Loss: 0.2754 | Val AUC: 0.8965 | Val Acc: 86.62%\n",
      "Epoch 12 | Train Loss: 0.2681 | Val AUC: 0.8995 | Val Acc: 85.71%\n",
      "Epoch 13 | Train Loss: 0.2619 | Val AUC: 0.8988 | Val Acc: 86.20%\n",
      "Epoch 14 | Train Loss: 0.2561 | Val AUC: 0.8973 | Val Acc: 86.12%\n",
      "Epoch 15 | Train Loss: 0.2504 | Val AUC: 0.8976 | Val Acc: 86.07%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m35\u001b[39m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m----> 5\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     val_metrics \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[45], line 189\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m    186\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    187\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;66;03m# batch['user_id'] shape : [Batch, 2] (pos, neg)\u001b[39;00m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;66;03m# On doit tout aplatir pour que le modèle voit une liste de samples [Batch * 2]\u001b[39;00m\n\u001b[1;32m    192\u001b[0m     user_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    193\u001b[0m     movie_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmovie_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    740\u001b[0m ):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    789\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    792\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[45], line 81\u001b[0m, in \u001b[0;36mMovieLensTrainDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 81\u001b[0m     u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43musers\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     82\u001b[0m     pos_m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmovies[idx]\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- 8. RUN ---\n",
    "EPOCHS = 35 \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_one_epoch(model, train_loader)\n",
    "    val_metrics = evaluate(model, val_loader)\n",
    "    \n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02d} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "        f\"Val AUC: {val_metrics['auc']:.4f} | \"\n",
    "        f\"Val Acc: {val_metrics['accuracy']*100:.2f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad57d571",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers (989051002.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[22], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Epoch 06 | Train Loss: 0.3018 | Val AUC: 0.8946 | Val Acc: 83.36%\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers\n"
     ]
    }
   ],
   "source": [
    "Epoch 01 | Train Loss: 0.4616 | Val AUC: 0.8428 | Val Acc: 73.09%\n",
    "Epoch 02 | Train Loss: 0.4041 | Val AUC: 0.8705 | Val Acc: 79.45%\n",
    "Epoch 03 | Train Loss: 0.3694 | Val AUC: 0.8830 | Val Acc: 82.48%\n",
    "Epoch 04 | Train Loss: 0.3493 | Val AUC: 0.8903 | Val Acc: 81.40%\n",
    "Epoch 05 | Train Loss: 0.3345 | Val AUC: 0.8939 | Val Acc: 83.13%\n",
    "Epoch 06 | Train Loss: 0.3229 | Val AUC: 0.8967 | Val Acc: 83.52%\n",
    "Epoch 07 | Train Loss: 0.3123 | Val AUC: 0.8968 | Val Acc: 83.95%\n",
    "Epoch 08 | Train Loss: 0.3010 | Val AUC: 0.8970 | Val Acc: 84.02%\n",
    "Epoch 09 | Train Loss: 0.2929 | Val AUC: 0.8986 | Val Acc: 86.05%\n",
    "Epoch 10 | Train Loss: 0.2835 | Val AUC: 0.8980 | Val Acc: 85.06%\n",
    "Epoch 11 | Train Loss: 0.2757 | Val AUC: 0.8987 | Val Acc: 84.57%\n",
    "Epoch 12 | Train Loss: 0.2689 | Val AUC: 0.8977 | Val Acc: 85.17%\n",
    "Epoch 13 | Train Loss: 0.2626 | Val AUC: 0.8996 | Val Acc: 86.23%\n",
    "Epoch 14 | Train Loss: 0.2569 | Val AUC: 0.9010 | Val Acc: 85.48%\n",
    "Epoch 15 | Train Loss: 0.2510 | Val AUC: 0.8987 | Val Acc: 86.22%\n",
    "Epoch 16 | Train Loss: 0.2464 | Val AUC: 0.9008 | Val Acc: 86.50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ed6863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Chargement des données...\n",
      "Modèle MLP créé avec 312545 paramètres.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# --- 1. SETUP & SEED ---\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed()\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() \n",
    "                 else \"cuda\" if torch.cuda.is_available() \n",
    "                 else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# --- 2. DATA LOADING (Strictement identique) ---\n",
    "def load_movielens(path=\"data/ml-1m/ratings.dat\"):\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            path, sep=\"::\", engine=\"python\", \n",
    "            names=[\"user_id\", \"movie_id\", \"rating\", \"timestamp\"]\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        print(\"Mode Démo : Génération de données synthétiques...\")\n",
    "        df = pd.DataFrame({\n",
    "            \"user_id\": np.random.randint(0, 100, 10000),\n",
    "            \"movie_id\": np.random.randint(0, 50, 10000),\n",
    "            \"rating\": np.random.randint(1, 6, 10000),\n",
    "            \"timestamp\": range(10000)\n",
    "        })\n",
    "    df[\"label\"] = (df[\"rating\"] >= 4).astype(int)\n",
    "    df = df[df[\"label\"] == 1]\n",
    "    return df\n",
    "\n",
    "def temporal_split(df):\n",
    "    df = df.sort_values([\"user_id\", \"timestamp\"])\n",
    "    train, val, test = [], [], []\n",
    "    for u, g in df.groupby(\"user_id\"):\n",
    "        if len(g) < 3:\n",
    "            train.append(g)\n",
    "        else:\n",
    "            train.append(g.iloc[:-2])\n",
    "            val.append(g.iloc[-2:-1])\n",
    "            test.append(g.iloc[-1:])\n",
    "    return pd.concat(train), pd.concat(val), pd.concat(test)\n",
    "\n",
    "def reindex(train_df, val_df, test_df):\n",
    "    users = pd.concat([train_df, val_df, test_df])[\"user_id\"].unique()\n",
    "    movies = pd.concat([train_df, val_df, test_df])[\"movie_id\"].unique()\n",
    "    user_map = {u: i for i, u in enumerate(users)}\n",
    "    movie_map = {m: i for i, m in enumerate(movies)}\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        df[\"user_id\"] = df[\"user_id\"].map(user_map)\n",
    "        df[\"movie_id\"] = df[\"movie_id\"].map(movie_map)\n",
    "    return train_df, val_df, test_df, user_map, movie_map\n",
    "\n",
    "# --- 3. DATASETS ---\n",
    "class MovieLensTrainDataset(Dataset):\n",
    "    def __init__(self, df, num_movies):\n",
    "        self.users = torch.tensor(df.user_id.values, dtype=torch.long)\n",
    "        self.movies = torch.tensor(df.movie_id.values, dtype=torch.long)\n",
    "        self.positives = set(zip(df.user_id, df.movie_id))\n",
    "        self.num_movies = num_movies\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        u = self.users[idx]\n",
    "        pos_m = self.movies[idx]\n",
    "        while True:\n",
    "            neg_m = random.randint(0, self.num_movies - 1)\n",
    "            if (u.item(), neg_m) not in self.positives:\n",
    "                break\n",
    "        return {\n",
    "            \"user_id\": torch.tensor([u, u]), \n",
    "            \"movie_id\": torch.tensor([pos_m, neg_m]),\n",
    "            \"label\": torch.tensor([1.0, 0.0])\n",
    "        }\n",
    "\n",
    "class MovieLensEvalDataset(Dataset):\n",
    "    def __init__(self, df, positives, num_movies, num_neg=99):\n",
    "        self.samples = []\n",
    "        for _, row in df.iterrows():\n",
    "            u, pos_m = row.user_id, row.movie_id\n",
    "            negs = []\n",
    "            while len(negs) < num_neg:\n",
    "                m = random.randint(0, num_movies - 1)\n",
    "                if (u, m) not in positives:\n",
    "                    negs.append(m)\n",
    "            self.samples.append((u, pos_m, negs))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        u, pos_m, negs = self.samples[idx]\n",
    "        movies = [pos_m] + negs\n",
    "        labels = [1] + [0] * len(negs)\n",
    "        return {\n",
    "            \"user_id\": torch.tensor([u] * len(movies)),\n",
    "            \"movie_id\": torch.tensor(movies),\n",
    "            \"label\": torch.tensor(labels, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "# --- 4. PREPARATION DATA ---\n",
    "print(\"Chargement des données...\")\n",
    "df = load_movielens() \n",
    "train_df, val_df, test_df = temporal_split(df)\n",
    "train_df, val_df, test_df, user_map, movie_map = reindex(train_df, val_df, test_df)\n",
    "\n",
    "num_users = len(user_map)\n",
    "num_movies = len(movie_map)\n",
    "\n",
    "train_ds = MovieLensTrainDataset(train_df, num_movies)\n",
    "val_ds = MovieLensEvalDataset(val_df, set(zip(train_df.user_id, train_df.movie_id)), num_movies, num_neg=19)\n",
    "train_loader = DataLoader(train_ds, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "# --- 5. LE MODÈLE MLP (BASELINE) ---\n",
    "class MLPRecModel(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embed_dim, hidden_layers=[64, 32], dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Embeddings (Mêmes dimensions que le DSN)\n",
    "        self.user_embedding = nn.Embedding(num_users, embed_dim)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embed_dim)\n",
    "        \n",
    "        # Initialisation standard (Xavier)\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.movie_embedding.weight)\n",
    "\n",
    "        # 2. Construction du MLP\n",
    "        # Input dim = Embed User + Embed Item\n",
    "        input_dim = embed_dim * 2\n",
    "        \n",
    "        layers = []\n",
    "        for hidden_dim in hidden_layers:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())  # Activation classique ReLU\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            input_dim = hidden_dim\n",
    "            \n",
    "        # Dernière couche de projection vers 1 seul neurone (le score)\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        \n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, user_ids, movie_ids):\n",
    "        u_emb = self.user_embedding(user_ids)\n",
    "        m_emb = self.movie_embedding(movie_ids)\n",
    "        \n",
    "        # Concaténation\n",
    "        x = torch.cat([u_emb, m_emb], dim=-1)\n",
    "        \n",
    "        # Passage dans le MLP\n",
    "        logits = self.mlp(x)\n",
    "        return logits\n",
    "\n",
    "# --- 6. CONFIGURATION ---\n",
    "# On garde les mêmes hyperparamètres pour être \"fair-play\"\n",
    "EMBED_DIM = 32\n",
    "# Le DSN avait [64, 32, 1], ici on définit les couches cachées [64, 32] \n",
    "# (la dernière couche vers 1 est gérée automatiquement dans la classe MLP)\n",
    "HIDDEN_DIM = [64, 32] \n",
    "\n",
    "model = MLPRecModel(\n",
    "    num_users=num_users,\n",
    "    num_movies=num_movies,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_layers=HIDDEN_DIM,\n",
    "    dropout=0.0\n",
    ").to(device)\n",
    "\n",
    "print(f\"Modèle MLP créé avec {sum(p.numel() for p in model.parameters())} paramètres.\")\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# --- 7. TRAINING LOOP ---\n",
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in loader:\n",
    "        user_ids = batch[\"user_id\"].view(-1).to(device)\n",
    "        movie_ids = batch[\"movie_id\"].view(-1).to(device)\n",
    "        labels = batch[\"label\"].view(-1).float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(user_ids, movie_ids).squeeze(-1)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "    return total_loss / (len(loader.dataset) * 2)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    for batch in loader:\n",
    "        user_ids = batch[\"user_id\"].view(-1).to(device)\n",
    "        movie_ids = batch[\"movie_id\"].view(-1).to(device)\n",
    "        labels = batch[\"label\"].view(-1).float().to(device)\n",
    "        \n",
    "        logits = model(user_ids, movie_ids).squeeze(-1)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        all_preds.append(probs.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    flat_preds = np.concatenate(all_preds)\n",
    "    flat_labels = np.concatenate(all_labels)\n",
    "    return {\n",
    "        \"auc\": roc_auc_score(flat_labels, flat_preds),\n",
    "        \"accuracy\": ((flat_preds > 0.5) == flat_labels).mean()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e683bac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DÉBUT DE L'ENTRAÎNEMENT MLP ---\n",
      "Epoch 01 | Train Loss: 0.3368 | Val AUC: 0.8907 | Val Acc: 82.05%\n",
      "Epoch 02 | Train Loss: 0.3314 | Val AUC: 0.8917 | Val Acc: 82.49%\n",
      "Epoch 03 | Train Loss: 0.3258 | Val AUC: 0.8933 | Val Acc: 82.96%\n",
      "Epoch 04 | Train Loss: 0.3214 | Val AUC: 0.8936 | Val Acc: 83.33%\n",
      "Epoch 05 | Train Loss: 0.3159 | Val AUC: 0.8946 | Val Acc: 84.12%\n",
      "Epoch 06 | Train Loss: 0.3115 | Val AUC: 0.8944 | Val Acc: 83.70%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- DÉBUT DE L\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mENTRAÎNEMENT MLP ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m----> 5\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     val_metrics \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[32], line 190\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m    188\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    189\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m    191\u001b[0m     user_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    192\u001b[0m     movie_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmovie_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    740\u001b[0m ):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    789\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    792\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[32], line 82\u001b[0m, in \u001b[0;36mMovieLensTrainDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (u\u001b[38;5;241m.\u001b[39mitem(), neg_m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositives:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([u, u]), \n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmovie_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpos_m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_m\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m])\n\u001b[1;32m     84\u001b[0m }\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- 8. RUN ---\n",
    "EPOCHS = 18 # On lance sur 11 époques comme ton précédent run\n",
    "print(\"\\n--- DÉBUT DE L'ENTRAÎNEMENT MLP ---\")\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_one_epoch(model, train_loader)\n",
    "    val_metrics = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | Val AUC: {val_metrics['auc']:.4f} | Val Acc: {val_metrics['accuracy']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fc8c02a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Chargement des données...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Import de ton architecture SBN\n",
    "# Si SBN.py n'est pas trouvé, assure-toi qu'il est dans le répertoire courant\n",
    "from SBN import SpectralBillinearNet\n",
    "\n",
    "# --- 1. SETUP & SEED ---\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                 else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# --- 2. DATA LOADING (Standardisé) ---\n",
    "def load_movielens(path=\"data/ml-1m/ratings.dat\"):\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            path, sep=\"::\", engine=\"python\", \n",
    "            names=[\"user_id\", \"movie_id\", \"rating\", \"timestamp\"]\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        print(\"Mode Démo : Génération de données synthétiques...\")\n",
    "        df = pd.DataFrame({\n",
    "            \"user_id\": np.random.randint(0, 100, 10000),\n",
    "            \"movie_id\": np.random.randint(0, 50, 10000),\n",
    "            \"rating\": np.random.randint(1, 6, 10000),\n",
    "            \"timestamp\": range(10000)\n",
    "        })\n",
    "    df[\"label\"] = (df[\"rating\"] >= 4).astype(int)\n",
    "    df = df[df[\"label\"] == 1]\n",
    "    return df\n",
    "\n",
    "def temporal_split(df):\n",
    "    df = df.sort_values([\"user_id\", \"timestamp\"])\n",
    "    train, val, test = [], [], []\n",
    "    for u, g in df.groupby(\"user_id\"):\n",
    "        if len(g) < 3:\n",
    "            train.append(g)\n",
    "        else:\n",
    "            train.append(g.iloc[:-2])\n",
    "            val.append(g.iloc[-2:-1])\n",
    "            test.append(g.iloc[-1:])\n",
    "    return pd.concat(train), pd.concat(val), pd.concat(test)\n",
    "\n",
    "def reindex(train_df, val_df, test_df):\n",
    "    users = pd.concat([train_df, val_df, test_df])[\"user_id\"].unique()\n",
    "    movies = pd.concat([train_df, val_df, test_df])[\"movie_id\"].unique()\n",
    "    user_map = {u: i for i, u in enumerate(users)}\n",
    "    movie_map = {m: i for i, m in enumerate(movies)}\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        df[\"user_id\"] = df[\"user_id\"].map(user_map)\n",
    "        df[\"movie_id\"] = df[\"movie_id\"].map(movie_map)\n",
    "    return train_df, val_df, test_df, user_map, movie_map\n",
    "\n",
    "# --- 3. DATASETS ---\n",
    "class MovieLensTrainDataset(Dataset):\n",
    "    def __init__(self, df, num_movies):\n",
    "        self.users = torch.tensor(df.user_id.values, dtype=torch.long)\n",
    "        self.movies = torch.tensor(df.movie_id.values, dtype=torch.long)\n",
    "        self.positives = set(zip(df.user_id, df.movie_id))\n",
    "        self.num_movies = num_movies\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        u = self.users[idx]\n",
    "        pos_m = self.movies[idx]\n",
    "        while True:\n",
    "            neg_m = random.randint(0, self.num_movies - 1)\n",
    "            if (u.item(), neg_m) not in self.positives:\n",
    "                break\n",
    "        return {\n",
    "            \"user_id\": torch.tensor([u, u]), \n",
    "            \"movie_id\": torch.tensor([pos_m, neg_m]),\n",
    "            \"label\": torch.tensor([1.0, 0.0])\n",
    "        }\n",
    "\n",
    "class MovieLensEvalDataset(Dataset):\n",
    "    def __init__(self, df, positives, num_movies, num_neg=99):\n",
    "        self.samples = []\n",
    "        for _, row in df.iterrows():\n",
    "            u, pos_m = row.user_id, row.movie_id\n",
    "            negs = []\n",
    "            while len(negs) < num_neg:\n",
    "                m = random.randint(0, num_movies - 1)\n",
    "                if (u, m) not in positives:\n",
    "                    negs.append(m)\n",
    "            self.samples.append((u, pos_m, negs))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        u, pos_m, negs = self.samples[idx]\n",
    "        movies = [pos_m] + negs\n",
    "        labels = [1] + [0] * len(negs)\n",
    "        return {\n",
    "            \"user_id\": torch.tensor([u] * len(movies)),\n",
    "            \"movie_id\": torch.tensor(movies),\n",
    "            \"label\": torch.tensor(labels, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "# --- 4. PREPARATION DATA ---\n",
    "print(\"Chargement des données...\")\n",
    "df = load_movielens() \n",
    "train_df, val_df, test_df = temporal_split(df)\n",
    "train_df, val_df, test_df, user_map, movie_map = reindex(train_df, val_df, test_df)\n",
    "\n",
    "num_users = len(user_map)\n",
    "num_movies = len(movie_map)\n",
    "\n",
    "train_ds = MovieLensTrainDataset(train_df, num_movies)\n",
    "val_ds = MovieLensEvalDataset(val_df, set(zip(train_df.user_id, train_df.movie_id)), num_movies, num_neg=19)\n",
    "train_loader = DataLoader(train_ds, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "# --- 5. LE MODÈLE SBN (SPECTRAL BILINEAR) WRAPPER ---\n",
    "class SBNRecModel(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embed_dim, layers_dim, ortho_mode='hard'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Embeddings (Identiques aux baselines précédentes)\n",
    "        self.user_embedding = nn.Embedding(num_users, embed_dim)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embed_dim)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.movie_embedding.weight)\n",
    "\n",
    "        # 2. Backbone SBN\n",
    "        # Input dim = Concat(User, Movie) -> embed_dim * 2\n",
    "        input_dim = embed_dim * 2\n",
    "        \n",
    "        # On insère la dimension d'entrée au début\n",
    "        full_layers_dim = [input_dim] + layers_dim\n",
    "        \n",
    "        self.backbone = DeepMultiBasisBilinearNet(\n",
    "            layers_dim=full_layers_dim,\n",
    "            ortho_mode=None,\n",
    "            num_bases=8,\n",
    "            use_final_linear=False, # Pour projeter vers le logit final sans activation bizarre\n",
    "            bias=True,\n",
    "            use_layernorm=True,     # SBN bénéficie souvent de LayerNorm\n",
    "            use_residual = True\n",
    "        )\n",
    "\n",
    "    def forward(self, user_ids, movie_ids):\n",
    "        u_emb = self.user_embedding(user_ids)\n",
    "        m_emb = self.movie_embedding(movie_ids)\n",
    "        \n",
    "        # Concaténation des features\n",
    "        x = torch.cat([u_emb, m_emb], dim=-1)\n",
    "        \n",
    "        # Passage dans le réseau bilinéaire spectral\n",
    "        logits = self.backbone(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# --- 7. TRAINING LOOP ---\n",
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in loader:\n",
    "        user_ids = batch[\"user_id\"].view(-1).to(device)\n",
    "        movie_ids = batch[\"movie_id\"].view(-1).to(device)\n",
    "        labels = batch[\"label\"].view(-1).float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(user_ids, movie_ids).squeeze(-1)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Si mode 'soft', ajouter la loss d'orthogonalité\n",
    "        # loss += 0.1 * model.backbone.get_total_ortho_loss(torch.nn.functional.mse_loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "    return total_loss / (len(loader.dataset) * 2)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    for batch in loader:\n",
    "        user_ids = batch[\"user_id\"].view(-1).to(device)\n",
    "        movie_ids = batch[\"movie_id\"].view(-1).to(device)\n",
    "        labels = batch[\"label\"].view(-1).float().to(device)\n",
    "        \n",
    "        logits = model(user_ids, movie_ids).squeeze(-1)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        all_preds.append(probs.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    flat_preds = np.concatenate(all_preds)\n",
    "    flat_labels = np.concatenate(all_labels)\n",
    "    return {\n",
    "        \"auc\": roc_auc_score(flat_labels, flat_preds),\n",
    "        \"accuracy\": ((flat_preds > 0.5) == flat_labels).mean()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "31bf623a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle SBN créé avec 1813041 paramètres.\n"
     ]
    }
   ],
   "source": [
    "# --- 6. CONFIGURATION ---\n",
    "EMBED_DIM = 64\n",
    "HIDDEN_LAYERS = [128,128,64,16, 1] # Output 1 pour la classification binaire\n",
    "\n",
    "model = SBNRecModel(\n",
    "    num_users=num_users,\n",
    "    num_movies=num_movies,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    layers_dim=HIDDEN_LAYERS,\n",
    "    ortho_mode=None # 'hard', 'cayley', ou 'soft'\n",
    ").to(device)\n",
    "\n",
    "print(f\"Modèle SBN créé avec {model.backbone.num_parameters + num_users*EMBED_DIM + num_movies*EMBED_DIM} paramètres.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d92ced68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DÉBUT DE L'ENTRAÎNEMENT SBN ---\n",
      "Epoch 01 | Train Loss: 0.4759 | Val AUC: 0.8335 | Val Acc: 74.99%\n",
      "Epoch 02 | Train Loss: 0.4326 | Val AUC: 0.8538 | Val Acc: 74.16%\n",
      "Epoch 03 | Train Loss: 0.3903 | Val AUC: 0.8746 | Val Acc: 80.18%\n",
      "Epoch 04 | Train Loss: 0.3665 | Val AUC: 0.8823 | Val Acc: 83.82%\n",
      "Epoch 05 | Train Loss: 0.3508 | Val AUC: 0.8881 | Val Acc: 81.06%\n",
      "Epoch 06 | Train Loss: 0.3374 | Val AUC: 0.8923 | Val Acc: 83.24%\n",
      "Epoch 07 | Train Loss: 0.3244 | Val AUC: 0.8940 | Val Acc: 80.98%\n",
      "Epoch 08 | Train Loss: 0.3136 | Val AUC: 0.8948 | Val Acc: 83.52%\n",
      "Epoch 09 | Train Loss: 0.3040 | Val AUC: 0.8957 | Val Acc: 84.48%\n",
      "Epoch 10 | Train Loss: 0.2953 | Val AUC: 0.8966 | Val Acc: 84.09%\n",
      "Epoch 11 | Train Loss: 0.2871 | Val AUC: 0.8981 | Val Acc: 83.59%\n",
      "Epoch 12 | Train Loss: 0.2798 | Val AUC: 0.8972 | Val Acc: 82.77%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[157], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m)\u001b[38;5;66;03m#, weight_decay=1e-6)  \u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m----> 7\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     val_metrics \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[155], line 182\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m    179\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    181\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 182\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmovie_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    183\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# Si mode 'soft', ajouter la loss d'orthogonalité\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# loss += 0.1 * model.backbone.get_total_ortho_loss(torch.nn.functional.mse_loss)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[155], line 165\u001b[0m, in \u001b[0;36mSBNRecModel.forward\u001b[0;34m(self, user_ids, movie_ids)\u001b[0m\n\u001b[1;32m    162\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([u_emb, m_emb], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# Passage dans le réseau bilinéaire spectral\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/spectral-neural-network-SNN/SBN.py:330\u001b[0m, in \u001b[0;36mDeepMultiBasisBilinearNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;66;03m# Gestion des connexions résiduelles pour les couches SBN\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, MultiBasisBilinearLayer):\n\u001b[0;32m--> 330\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;66;03m# Résiduel seulement si les dimensions matchent\u001b[39;00m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_residual \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m out\u001b[38;5;241m.\u001b[39mshape:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/spectral-neural-network-SNN/SBN.py:255\u001b[0m, in \u001b[0;36mMultiBasisBilinearLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    252\u001b[0m combined_interactions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(all_interactions, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# 3. Combinaison linéaire finale\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meigen_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_interactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- 8. RUN ---\n",
    "EPOCHS = 25 \n",
    "print(\"\\n--- DÉBUT DE L'ENTRAÎNEMENT SBN ---\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)#, weight_decay=1e-6)  \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_one_epoch(model, train_loader)\n",
    "    val_metrics = evaluate(model, val_loader)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02d} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "        f\"Val AUC: {val_metrics['auc']:.4f} | \"\n",
    "        f\"Val Acc: {val_metrics['accuracy']*100:.2f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5dca83c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INSPECTION SBN (Couche 0) : User 2 & Movie 1193 ---\n",
      "Score Final du Modèle : 0.92 (Prob: 71.5%)\n",
      "--------------------------------------------------\n",
      "Réaction des 8 bases (Concepts) :\n",
      "Base #5 : -31.6990 | 🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥\n",
      "Base #4 : -27.3431 | 🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥\n",
      "Base #1 : -17.8344 | 🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥\n",
      "Base #6 : -12.0690 | 🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥\n",
      "Base #7 : -10.8471 | 🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥\n",
      "Base #0 : +8.1263 | 🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩\n",
      "Base #2 : -5.3683 | 🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥\n",
      "Base #3 : -4.1685 | 🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥\n"
     ]
    }
   ],
   "source": [
    "def inspect_sbn_concepts(model, user_id, movie_id, layer_idx=0):\n",
    "    \"\"\"\n",
    "    Inspecte l'activation des bases bilinéaires (Concepts) pour une paire User-Movie.\n",
    "    Affiche la 'Résonance' de chaque base : à quel point le modèle réagit-il ?\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Prépare les inputs\n",
    "    u_tensor = torch.tensor([user_id]).to(device)\n",
    "    m_tensor = torch.tensor([movie_id]).to(device)\n",
    "    \n",
    "    # 2. Embeddings & Concat\n",
    "    u_emb = model.user_embedding(u_tensor)\n",
    "    m_emb = model.movie_embedding(m_tensor)\n",
    "    x = torch.cat([u_emb, m_emb], dim=-1) # [1, Dim]\n",
    "    \n",
    "    # 3. Récupération de la couche ciblée\n",
    "    # On cherche la couche SBN correspondante dans le backbone\n",
    "    sbn_layers = [l for l in model.backbone.layers if hasattr(l, 'num_bases')]\n",
    "    \n",
    "    if len(sbn_layers) <= layer_idx:\n",
    "        print(\"Index de couche invalide ou pas de couche SBN trouvée.\")\n",
    "        return\n",
    "        \n",
    "    layer = sbn_layers[layer_idx]\n",
    "    \n",
    "    print(f\"--- INSPECTION SBN (Couche {layer_idx}) : User {user_id} & Movie {movie_id} ---\")\n",
    "    \n",
    "    # Score Final pour contexte\n",
    "    with torch.no_grad():\n",
    "        logit = model(u_tensor, m_tensor).item()\n",
    "        prob = torch.sigmoid(torch.tensor(logit)).item()\n",
    "    print(f\"Score Final du Modèle : {logit:.2f} (Prob: {prob*100:.1f}%)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    activations = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(layer.num_bases):\n",
    "            # A. Projections\n",
    "            right = layer.right_bases[i](x)\n",
    "            left = layer.left_bases[i](x)\n",
    "            \n",
    "            # B. Énergie d'interaction (Produit scalaire implicite)\n",
    "            # Une valeur élevée signifie que les vecteurs Right et Left sont alignés\n",
    "            # C'est la \"force\" de la réaction de cette base\n",
    "            interaction = (right * left).mean().item()\n",
    "            \n",
    "            activations.append((i, interaction))\n",
    "            \n",
    "    # 4. Tri par intensité d'activation (Absolue)\n",
    "    activations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    print(f\"Réaction des {layer.num_bases} bases (Concepts) :\")\n",
    "    for base_idx, energy in activations:\n",
    "        # Visuel\n",
    "        bar_len = int(abs(energy) * 50) # Facteur d'échelle pour l'affichage\n",
    "        if energy > 0:\n",
    "            bar = \"🟩\" * bar_len\n",
    "            sign = \"+\"\n",
    "        else:\n",
    "            bar = \"🟥\" * bar_len\n",
    "            sign = \"-\"\n",
    "            \n",
    "        print(f\"Base #{base_idx} : {sign}{abs(energy):.4f} | {bar}\")\n",
    "\n",
    "# Test sur la couche 0 (la première couche conceptuelle)\n",
    "inspect_sbn_concepts(model, user_id=2, movie_id=1193, layer_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fc649612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ANALYSE D'IMPACT (Ablation) : User 2 & Movie 1193 ---\n",
      "Score Original : 0.9183 (Prob: 71.5%)\n",
      "------------------------------------------------------------\n",
      "Base #7 : -0.4520 (PENALISANT) 🟥🟥🟥🟥🟥🟥🟥🟥🟥\n",
      "Base #3 : +0.4152 (AIDANT) 🟩🟩🟩🟩🟩🟩🟩🟩\n",
      "Base #5 : +0.3664 (AIDANT) 🟩🟩🟩🟩🟩🟩🟩\n",
      "Base #0 : -0.2508 (PENALISANT) 🟥🟥🟥🟥🟥\n",
      "Base #1 : -0.1329 (PENALISANT) 🟥🟥\n",
      "Base #6 : +0.1101 (AIDANT) 🟩🟩\n",
      "Base #4 : +0.0755 (AIDANT) 🟩\n",
      "Base #2 : +0.0177 (AIDANT) \n"
     ]
    }
   ],
   "source": [
    "def explain_by_ablation(model, user_id, movie_id, layer_idx=0):\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Préparer les inputs\n",
    "    u_tensor = torch.tensor([user_id]).to(device)\n",
    "    m_tensor = torch.tensor([movie_id]).to(device)\n",
    "    \n",
    "    # 2. Score original (Référence)\n",
    "    with torch.no_grad():\n",
    "        original_logit = model(u_tensor, m_tensor).item()\n",
    "        original_prob = torch.sigmoid(torch.tensor(original_logit)).item()\n",
    "    \n",
    "    print(f\"--- ANALYSE D'IMPACT (Ablation) : User {user_id} & Movie {movie_id} ---\")\n",
    "    print(f\"Score Original : {original_logit:.4f} (Prob: {original_prob*100:.1f}%)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # 3. Accéder à la couche cible\n",
    "    sbn_layers = [l for l in model.backbone.layers if hasattr(l, 'num_bases')]\n",
    "    target_layer = sbn_layers[layer_idx]\n",
    "    \n",
    "    impacts = []\n",
    "    \n",
    "    # 4. On teste chaque base une par une\n",
    "    # Pour faire ça proprement sans modifier le code du modèle, on va utiliser un \"Hook\"\n",
    "    # ou plus simplement : on fait le forward manuellement jusqu'à la couche, on modifie, et on continue.\n",
    "    # MAIS pour faire simple et robuste : on va patcher temporairement la méthode forward de la couche.\n",
    "    \n",
    "    original_forward = target_layer.forward\n",
    "    \n",
    "    try:\n",
    "        for i in range(target_layer.num_bases):\n",
    "            \n",
    "            # On définit un forward modifié qui annule JUSTE la base i\n",
    "            def ablated_forward(x, base_idx_to_kill=i, original_fwd=original_forward):\n",
    "                # Recopier la logique interne de la couche SBN MultiBasis\n",
    "                # C'est un peu intrusif, mais c'est le seul moyen précis sans réécrire tout le forward du modèle\n",
    "                \n",
    "                # Note: On suppose ici que c'est une MultiBasisBilinearLayer comme dans ton SBN_Multi.py\n",
    "                all_interactions = []\n",
    "                for b in range(target_layer.num_bases):\n",
    "                    right = target_layer.right_bases[b](x)\n",
    "                    left = target_layer.left_bases[b](x)\n",
    "                    interaction = right * left\n",
    "                    \n",
    "                    # C'EST ICI LA MAGIE : On tue l'interaction si c'est la base cible\n",
    "                    if b == base_idx_to_kill:\n",
    "                        interaction = torch.zeros_like(interaction)\n",
    "                        \n",
    "                    all_interactions.append(interaction)\n",
    "                \n",
    "                combined = torch.cat(all_interactions, dim=-1)\n",
    "                return target_layer.eigen_weights(combined)\n",
    "\n",
    "            # On remplace temporairement la méthode\n",
    "            target_layer.forward = ablated_forward\n",
    "            \n",
    "            # On recalcule le score\n",
    "            with torch.no_grad():\n",
    "                new_logit = model(u_tensor, m_tensor).item()\n",
    "            \n",
    "            # Impact = Différence (Score Original - Score Ablaté)\n",
    "            # Si Impact > 0 : La base \"ajoutait\" du score (C'est un motif POSITIF)\n",
    "            # Si Impact < 0 : La base \"enlevait\" du score (C'est un motif NEGATIF)\n",
    "            impact = original_logit - new_logit\n",
    "            impacts.append((i, impact))\n",
    "            \n",
    "    finally:\n",
    "        # TRES IMPORTANT : Remettre le forward original quoi qu'il arrive\n",
    "        target_layer.forward = original_forward\n",
    "\n",
    "    # 5. Affichage trié\n",
    "    impacts.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    for base_idx, impact in impacts:\n",
    "        if abs(impact) < 0.001: continue # On ignore le bruit\n",
    "        \n",
    "        direction = \"AIDANT\" if impact > 0 else \"PENALISANT\"\n",
    "        bar_char = \"🟩\" if impact > 0 else \"🟥\"\n",
    "        bar = bar_char * int(abs(impact) * 20) # Echelle visuelle\n",
    "        \n",
    "        print(f\"Base #{base_idx} : {impact:+.4f} ({direction}) {bar}\")\n",
    "\n",
    "# Test\n",
    "explain_by_ablation(model, user_id=2, movie_id=1193)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1e91b248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PROFILAGE DU CONCEPT (BASE #6) ---\n",
      "Films qui définissent ce concept :\n",
      "  - Movie ID 1961 (Force: 171.9574)\n",
      "  - Movie ID 2858 (Force: 168.3036)\n",
      "  - Movie ID 1580 (Force: 165.9115)\n",
      "  - Movie ID 1210 (Force: 164.1475)\n",
      "  - Movie ID 34 (Force: 159.4895)\n"
     ]
    }
   ],
   "source": [
    "def profile_base_concept(model, base_idx, movie_map, k=5):\n",
    "    \"\"\"\n",
    "    Affiche les films qui activent le plus (positivement ou négativement) une base donnée.\n",
    "    Permet de comprendre sémantiquement ce que la base a appris.\n",
    "    \"\"\"\n",
    "    # Inverser la map pour avoir ID -> Titre (si tu as les titres, sinon juste ID)\n",
    "    # Ici on utilise les IDs bruts\n",
    "    id_to_movie = {v: k for k, v in movie_map.items()} \n",
    "    \n",
    "    print(f\"--- PROFILAGE DU CONCEPT (BASE #{base_idx}) ---\")\n",
    "    \n",
    "    # On va tester la base sur un échantillon de films (ou tous)\n",
    "    # Pour aller vite, on prend les 100 premiers films du map\n",
    "    sample_movies = list(movie_map.values())[:500]\n",
    "    \n",
    "    activations = []\n",
    "    layer = model.backbone.layers[0] # On cible la couche SBN\n",
    "    \n",
    "    # On crée un faux batch d'utilisateurs (on s'intéresse à la caractéristique intrinsèque du film)\n",
    "    # On peut utiliser une moyenne d'utilisateurs ou un utilisateur neutre (embedding nul)\n",
    "    # Ici on regarde juste la projection \"Gauche\" du film (sa caractéristique latente)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        movie_tensor = torch.tensor(sample_movies).to(device)\n",
    "        m_emb = model.movie_embedding(movie_tensor)\n",
    "        \n",
    "        # On projette via la matrice LEFT de la base cible\n",
    "        # (Dans le SBN, Right=User, Left=Movie ou inversement selon l'interaction)\n",
    "        # On regarde la norme de la projection pour voir l'intensité\n",
    "        proj = layer.left_bases[base_idx](torch.cat([torch.zeros_like(m_emb), m_emb], dim=-1))\n",
    "        \n",
    "        # On prend la norme comme \"force\" du signal\n",
    "        energies = proj.norm(dim=1)\n",
    "        \n",
    "        # Top films\n",
    "        top_indices = torch.topk(energies, k).indices\n",
    "        \n",
    "        print(\"Films qui définissent ce concept :\")\n",
    "        for idx in top_indices:\n",
    "            movie_id = sample_movies[idx.item()]\n",
    "            raw_id = id_to_movie.get(movie_id, \"Unknown\")\n",
    "            print(f\"  - Movie ID {raw_id} (Force: {energies[idx]:.4f})\")\n",
    "\n",
    "# Exemple : Qu'est-ce que la Base #6 (ton moteur principal) ?\n",
    "profile_base_concept(model, base_idx=6, movie_map=movie_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5b201c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DSN import *\n",
    "# --- 2. Le Bloc Transformer Spectral (Le LEGO) ---\n",
    "class SpectralTransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1, ortho_mode=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # A. Attention Temporelle (Standard)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # B. Logique Spectrale (Ton Innovation SBN)\n",
    "        # Remplace le gros FeedForward (d_model -> 4*d_model -> d_model)\n",
    "        self.spectral_ffn = MultiBasisSpectralLayer(\n",
    "            in_features=d_model,\n",
    "            out_features=d_model,\n",
    "            num_bases=4,          # Tu peux augmenter à 8 pour plus de puissance\n",
    "            ortho_mode=ortho_mode\n",
    "        )\n",
    "        \n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: [Batch, Seq_Len, Dim]\n",
    "        \n",
    "        # 1. Self-Attention (Gère l'historique)\n",
    "        attn_out, _ = self.attention(x, x, x, attn_mask=mask, need_weights=False)\n",
    "        x = self.ln1(x + self.dropout1(attn_out))\n",
    "        \n",
    "        # 2. Spectral Bilinear Network (Gère le raisonnement)\n",
    "        spectral_out = self.spectral_ffn(x)\n",
    "        x = self.ln2(x + self.dropout2(spectral_out))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class SASRecSpectral(nn.Module):\n",
    "    def __init__(self, num_users, num_items, max_len, d_model=64, num_heads=2, num_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.item_emb = nn.Embedding(num_items, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # --- C'EST ICI QUE TU CHANGES TOUT ---\n",
    "        # Au lieu de nn.TransformerEncoderLayer, tu utilises TA classe\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SpectralTransformerBlock(d_model, num_heads, ortho_mode='soft')\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, item_seq):\n",
    "        # item_seq: [Batch, Seq_Len]\n",
    "        seq_len = item_seq.size(1)\n",
    "        \n",
    "        # Création du masque causal (pour ne pas voir le futur)\n",
    "        # mask = ... (code standard PyTorch pour masque triangulaire)\n",
    "        \n",
    "        # Embeddings + Positional\n",
    "        positions = torch.arange(seq_len, device=item_seq.device).unsqueeze(0)\n",
    "        x = self.item_emb(item_seq) + self.pos_emb(positions)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Passage dans tes blocs spectraux\n",
    "        for block in self.blocks:\n",
    "            # Note: Ajoute le masque d'attention ici si nécessaire\n",
    "            x = block(x)\n",
    "            \n",
    "        x = self.final_norm(x)\n",
    "        return x # [Batch, Seq_Len, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "41458312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 1. Fonction pour générer les séquences\n",
    "def get_sequential_data(df, min_len=3):\n",
    "    \"\"\"\n",
    "    Groupe par user et trie par temps pour créer des séquences [Film1, Film2, ...]\n",
    "    \"\"\"\n",
    "    # Tri vital pour le séquentiel\n",
    "    df = df.sort_values(by=['user_id', 'timestamp'])\n",
    "    \n",
    "    user_groups = df.groupby('user_id')\n",
    "    sequences = []\n",
    "    \n",
    "    for user_id, group in user_groups:\n",
    "        movie_ids = group['movie_id'].tolist()\n",
    "        \n",
    "        # On ne garde que si l'utilisateur a vu assez de films\n",
    "        if len(movie_ids) < min_len:\n",
    "            continue\n",
    "            \n",
    "        # Format : (User, Sequence_Historique, Target_Item)\n",
    "        # Pour l'entrainement SASRec classique, on prend tout l'historique sauf le dernier comme input\n",
    "        # et le dernier comme target \"positive\".\n",
    "        \n",
    "        # Train : Tout sauf les 2 derniers\n",
    "        train_seq = movie_ids[:-2]\n",
    "        train_target = movie_ids[-2]\n",
    "        sequences.append((train_seq, train_target))\n",
    "        \n",
    "        # (Pour faire simple ici, on ne fait pas le split complexe Val/Test temporel \n",
    "        # mais tu peux adapter si tu veux être rigoureux comme avant)\n",
    "\n",
    "    return sequences\n",
    "\n",
    "# 2. Le Dataset Séquentiel\n",
    "class SequentialDataset(Dataset):\n",
    "    def __init__(self, sequences, num_items, max_len=50):\n",
    "        self.sequences = sequences\n",
    "        self.num_items = num_items\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq, pos_target = self.sequences[idx]\n",
    "        \n",
    "        # Gestion de la longueur (Padding ou Truncate)\n",
    "        # On prend les 'max_len' derniers films (le passé récent)\n",
    "        seq = seq[-self.max_len:]\n",
    "        \n",
    "        # Négatif Sampling (pour l'entrainement)\n",
    "        # On choisit un film au hasard que l'user n'a (probablement) pas vu\n",
    "        while True:\n",
    "            neg_target = random.randint(0, self.num_items - 1)\n",
    "            if neg_target != pos_target and neg_target not in seq:\n",
    "                break\n",
    "                \n",
    "        return {\n",
    "            \"seq\": torch.tensor(seq, dtype=torch.long),\n",
    "            \"pos\": torch.tensor(pos_target, dtype=torch.long),\n",
    "            \"neg\": torch.tensor(neg_target, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 3. Collate Function (Indispensable pour gérer les tailles variables)\n",
    "def collate_fn(batch):\n",
    "    # Padding des séquences pour qu'elles aient la même taille dans le batch\n",
    "    seqs = [item['seq'] for item in batch]\n",
    "    pos = torch.stack([item['pos'] for item in batch])\n",
    "    neg = torch.stack([item['neg'] for item in batch])\n",
    "    \n",
    "    # Pad avec 0 (ou un ID spécial padding si tu en as un, ici on suppose que 0 est un film valide \n",
    "    # donc idéalement faudrait décaler tous les IDs de +1, mais pour le test ça ira)\n",
    "    seqs_padded = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return {\n",
    "        \"seq\": seqs_padded, \n",
    "        \"pos\": pos, \n",
    "        \"neg\": neg\n",
    "    }\n",
    "\n",
    "class SASRecWrapper(nn.Module):\n",
    "    def __init__(self, sasrec_model):\n",
    "        super().__init__()\n",
    "        self.model = sasrec_model\n",
    "        \n",
    "    def forward(self, seq, pos_items, neg_items):\n",
    "        # 1. On passe la séquence dans le Spectral Transformer\n",
    "        # Output : [Batch, Seq_Len, Dim]\n",
    "        # On s'intéresse uniquement au DERNIER état de la séquence (le résumé de l'historique)\n",
    "        log_feats = self.model(seq) \n",
    "        final_feat = log_feats[:, -1, :] # [Batch, Dim]\n",
    "        \n",
    "        # 2. On récupère les embeddings des items cibles (Pos et Neg)\n",
    "        # item_emb est dans self.model.item_emb\n",
    "        pos_emb = self.model.item_emb(pos_items) # [Batch, Dim]\n",
    "        neg_emb = self.model.item_emb(neg_items) # [Batch, Dim]\n",
    "        \n",
    "        # 3. Calcul des scores (Produit Scalaire)\n",
    "        pos_logits = (final_feat * pos_emb).sum(dim=-1) # Score de compatibilité Positif\n",
    "        neg_logits = (final_feat * neg_emb).sum(dim=-1) # Score de compatibilité Négatif\n",
    "        \n",
    "        return pos_logits, neg_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e8d1ffbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouveau vocabulaire items : 3534 (0 réservé padding)\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Reconstruction du DF Réindexé Propre ---\n",
    "# On concatène tes dataframes déjà réindexés pour avoir tout l'historique propre\n",
    "df_reindexed = pd.concat([train_df, val_df, test_df]).sort_values(['user_id', 'timestamp'])\n",
    "\n",
    "# IMPORTANT : On décale tous les IDs de +1\n",
    "# Pourquoi ? Parce que le \"Pad Sequence\" utilise 0 pour le vide.\n",
    "# Si un film a l'ID 0, le modèle croira que c'est du vide !\n",
    "# Donc : 0 = Padding, 1 = Film 0, 2 = Film 1...\n",
    "df_reindexed['movie_id'] = df_reindexed['movie_id'] + 1\n",
    "num_items_seq = num_movies + 1 # On agrandit le vocabulaire de 1\n",
    "\n",
    "print(f\"Nouveau vocabulaire items : {num_items_seq} (0 réservé padding)\")\n",
    "\n",
    "# --- 2. Génération des Séquences avec le bon DF ---\n",
    "# On utilise df_reindexed au lieu de df\n",
    "seq_data = get_sequential_data(df_reindexed) \n",
    "\n",
    "# --- 3. Dataset & Loader ---\n",
    "# On passe num_items_seq au dataset\n",
    "train_ds = SequentialDataset(seq_data, num_items_seq, max_len=50)\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# --- 4. Modèle Adapté ---\n",
    "# Attention : num_items = num_items_seq maintenant\n",
    "base_model = SASRecSpectral(\n",
    "    num_users=num_users, \n",
    "    num_items=num_items_seq, # Taille augmentée\n",
    "    max_len=50, \n",
    "    d_model=64, \n",
    "    num_layers=2\n",
    ").to(device)\n",
    "\n",
    "# Padding Index pour l'embedding (Optimisation)\n",
    "# On dit à PyTorch que l'index 0 est du vide (ne pas apprendre dessus)\n",
    "base_model.item_emb = nn.Embedding(num_items_seq, 64, padding_idx=0).to(device)\n",
    "\n",
    "model = SASRecWrapper(base_model).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# --- Fonction d'entraînement (1 Epoque) avec Métriques ---\n",
    "def train_sasrec_epoch_with_metrics(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Listes pour stocker les prédictions de toute l'époque (pour l'AUC global)\n",
    "    all_pos_scores = []\n",
    "    all_neg_scores = []\n",
    "    \n",
    "    for batch in loader:\n",
    "        seq = batch['seq'].to(device)\n",
    "        pos = batch['pos'].to(device)\n",
    "        neg = batch['neg'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 1. Forward : On récupère les scores (logits)\n",
    "        pos_logits, neg_logits = model(seq, pos, neg)\n",
    "        \n",
    "        # 2. Loss (BCE)\n",
    "        # On veut que pos_logits soit proche de 1 et neg_logits proche de 0\n",
    "        pos_labels = torch.ones_like(pos_logits)\n",
    "        neg_labels = torch.zeros_like(neg_logits)\n",
    "        \n",
    "        loss = criterion(pos_logits, pos_labels) + criterion(neg_logits, neg_labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # 3. Stockage pour métriques (On détache du graphe pour ne pas saturer la RAM)\n",
    "        # On applique Sigmoid pour avoir des probabilités entre 0 et 1\n",
    "        with torch.no_grad():\n",
    "            pos_probs = torch.sigmoid(pos_logits).cpu().numpy()\n",
    "            neg_probs = torch.sigmoid(neg_logits).cpu().numpy()\n",
    "            all_pos_scores.append(pos_probs)\n",
    "            all_neg_scores.append(neg_probs)\n",
    "            \n",
    "    # --- Calcul des Métriques en fin d'époque ---\n",
    "    # On concatène tout\n",
    "    all_pos = np.concatenate(all_pos_scores)\n",
    "    all_neg = np.concatenate(all_neg_scores)\n",
    "    \n",
    "    # A. Accuracy (Hit Rate) : Est-ce que Score(Pos) > Score(Neg) ?\n",
    "    # C'est la métrique la plus intuitive : \"Le modèle a-t-il préféré le bon film ?\"\n",
    "    accuracy = (all_pos > all_neg).mean()\n",
    "    \n",
    "    # B. AUC : On compare tous les 1 vs tous les 0\n",
    "    # On crée un grand vecteur [Probs_Pos, Probs_Neg] et un vecteur [1, 1..., 0, 0...]\n",
    "    y_scores = np.concatenate([all_pos, all_neg])\n",
    "    y_true = np.concatenate([np.ones_like(all_pos), np.zeros_like(all_neg)])\n",
    "    \n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "    \n",
    "    return total_loss / len(loader), auc, accuracy\n",
    "\n",
    "# --- Lancement ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2eedb031",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SASRecWithSpectralHead(nn.Module):\n",
    "    def __init__(self, sasrec_model, d_model=64, ortho_mode='soft'):\n",
    "        super().__init__()\n",
    "        self.backbone = sasrec_model # Ton SASRecSpectral\n",
    "        \n",
    "        # --- L'INNOVATION : La Tête Spectrale (Comme ton DSN statique) ---\n",
    "        # Elle prend [Contexte_User (64) + Item_Cible (64)] -> Sort 1 Score\n",
    "        self.prediction_head = MultiBasisSpectralLayer(\n",
    "            in_features=d_model * 2, # Concaténation\n",
    "            out_features=1,          # Score final\n",
    "            num_bases=4,\n",
    "            ortho_mode=ortho_mode\n",
    "        )\n",
    "        \n",
    "    def forward(self, seq, pos_items, neg_items):\n",
    "        # 1. Extraction du Contexte (Via le Transformer)\n",
    "        # [Batch, Seq_Len, Dim]\n",
    "        log_feats = self.backbone(seq) \n",
    "        \n",
    "        # On prend le dernier état (le résumé de l'historique)\n",
    "        user_context = log_feats[:, -1, :] # [Batch, Dim]\n",
    "        \n",
    "        # 2. Récupération des Embeddings Items\n",
    "        pos_emb = self.backbone.item_emb(pos_items) # [Batch, Dim]\n",
    "        neg_emb = self.backbone.item_emb(neg_items) # [Batch, Dim]\n",
    "        \n",
    "        # 3. INTERACTION SPECTRALE (Au lieu du Dot Product)\n",
    "        \n",
    "        # Pour le Positif\n",
    "        # On concatène le contexte utilisateur et l'item candidat\n",
    "        pos_input = torch.cat([user_context, pos_emb], dim=-1) # [Batch, Dim*2]\n",
    "        pos_logits = self.prediction_head(pos_input).squeeze(-1)\n",
    "        \n",
    "        # Pour le Négatif\n",
    "        neg_input = torch.cat([user_context, neg_emb], dim=-1) # [Batch, Dim*2]\n",
    "        neg_logits = self.prediction_head(neg_input).squeeze(-1)\n",
    "        \n",
    "        return pos_logits, neg_logits\n",
    "\n",
    "# --- MISE EN PLACE ---\n",
    "\n",
    "# 1. On garde ton modèle de base (le Transformer)\n",
    "base_model = SASRecSpectral(\n",
    "    num_users=num_users, \n",
    "    num_items=num_items_seq, \n",
    "    max_len=50, \n",
    "    d_model=64, \n",
    "    num_layers=2\n",
    ").to(device)\n",
    "\n",
    "# 2. On l'emballe avec la TÊTE SPECTRALE (Le \"Cerveau\" de décision)\n",
    "model = SASRecWithSpectralHead(base_model, d_model=64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8658d07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Lancement du Turbo Training sur 30 époques...\n",
      "Epoch 01 | Loss: 0.2095 | AUC: 0.9904 | Acc: 98.74%\n",
      "Epoch 02 | Loss: 0.2203 | AUC: 0.9916 | Acc: 98.96%\n",
      "Epoch 03 | Loss: 0.2229 | AUC: 0.9899 | Acc: 98.79%\n",
      "Epoch 04 | Loss: 0.2231 | AUC: 0.9898 | Acc: 98.79%\n",
      "Epoch 05 | Loss: 0.2438 | AUC: 0.9888 | Acc: 98.56%\n",
      "Epoch 06 | Loss: 0.2492 | AUC: 0.9867 | Acc: 98.36%\n",
      "Epoch 07 | Loss: 0.2451 | AUC: 0.9880 | Acc: 98.49%\n",
      "Epoch 08 | Loss: 0.2792 | AUC: 0.9850 | Acc: 98.29%\n",
      "Epoch 09 | Loss: 0.2835 | AUC: 0.9846 | Acc: 98.36%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m neg \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 29\u001b[0m pos_logits, neg_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pos_logits, torch\u001b[38;5;241m.\u001b[39mones_like(pos_logits)) \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m     32\u001b[0m        criterion(neg_logits, torch\u001b[38;5;241m.\u001b[39mzeros_like(neg_logits))\n\u001b[1;32m     34\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[78], line 18\u001b[0m, in \u001b[0;36mSASRecWithSpectralHead.forward\u001b[0;34m(self, seq, pos_items, neg_items)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, seq, pos_items, neg_items):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# 1. Extraction du Contexte (Via le Transformer)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# [Batch, Seq_Len, Dim]\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     log_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# On prend le dernier état (le résumé de l'historique)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     user_context \u001b[38;5;241m=\u001b[39m log_feats[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;66;03m# [Batch, Dim]\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[60], line 70\u001b[0m, in \u001b[0;36mSASRecSpectral.forward\u001b[0;34m(self, item_seq)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Passage dans tes blocs spectraux\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# Note: Ajoute le masque d'attention ici si nécessaire\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_norm(x)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[60], line 28\u001b[0m, in \u001b[0;36mSpectralTransformerBlock.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# x: [Batch, Seq_Len, Dim]\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# 1. Self-Attention (Gère l'historique)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     attn_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(attn_out))\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# 2. Spectral Bilinear Network (Gère le raisonnement)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/activation.py:1380\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1354\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1355\u001b[0m         query,\n\u001b[1;32m   1356\u001b[0m         key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1377\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m   1378\u001b[0m     )\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1380\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1389\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1393\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1398\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1400\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/functional.py:6370\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6365\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m bias_v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   6367\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   6368\u001b[0m \u001b[38;5;66;03m# reshape q, k, v for multihead attention and make them batch first\u001b[39;00m\n\u001b[1;32m   6369\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m-> 6370\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m(tgt_len, bsz \u001b[38;5;241m*\u001b[39m num_heads, head_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   6371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m static_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   6372\u001b[0m     k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mview(k\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], bsz \u001b[38;5;241m*\u001b[39m num_heads, head_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 1. Config plus musclée\n",
    "EPOCHS = 30 # Minimum pour voir la convergence\n",
    "LR = 1e-3\n",
    "\n",
    "# 2. Scheduler \"OneCycle\" (Le secret des Transformers qui convergent vite)\n",
    "# Il commence doucement, accélère, puis ralentit pour affiner.\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5) # Moins de decay\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, \n",
    "    max_lr=LR, \n",
    "    steps_per_epoch=len(train_loader), \n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "print(f\"🚀 Lancement du Turbo Training sur {EPOCHS} époques...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        seq = batch['seq'].to(device)\n",
    "        pos = batch['pos'].to(device)\n",
    "        neg = batch['neg'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pos_logits, neg_logits = model(seq, pos, neg)\n",
    "        \n",
    "        loss = criterion(pos_logits, torch.ones_like(pos_logits)) + \\\n",
    "               criterion(neg_logits, torch.zeros_like(neg_logits))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step() # On met à jour le LR à chaque batch\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # Evaluation (une fois par époque)\n",
    "    if (epoch + 1) % 1 == 0: # On affiche tous les 5 pour pas spammer\n",
    "        _, auc, acc = train_sasrec_epoch_with_metrics(model, train_loader) # Ou val_loader idéalement\n",
    "        print(f\"Epoch {epoch+1:02d} | Loss: {avg_loss:.4f} | AUC: {auc:.4f} | Acc: {acc*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1:02d} | Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae16fe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "🚀 Lancement du Turbo Training sur 30 époques...Epoch 01 | Loss: 2.1883Epoch 02 | Loss: 1.8683Epoch 03 | Loss: 1.6445Epoch 04 | Loss: 1.4915Epoch 05 | Loss: 1.3841 | AUC: 0.6765 | Acc: 67.66%Epoch 06 | Loss: 1.2368Epoch 07 | Loss: 1.2088Epoch 08 | Loss: 1.1580Epoch 09 | Loss: 1.1347Epoch 10 | Loss: 1.1020 | AUC: 0.8091 | Acc: 81.39%Epoch 11 | Loss: 1.0144Epoch 12 | Loss: 1.0165Epoch 13 | Loss: 0.9825Epoch 14 | Loss: 0.9499Epoch 15 | Loss: 0.9301 | AUC: 0.8610 | Acc: 86.35%Epoch 16 | Loss: 0.8868Epoch 17 | Loss: 0.8991Epoch 18 | Loss: 0.8595Epoch 19 | Loss: 0.8326Epoch 20 | Loss: 0.8277 | AUC: 0.8923 | Acc: 89.53%Epoch 21 | Loss: 0.7890Epoch 22 | Loss: 0.7886Epoch 23 | Loss: 0.7530Epoch 24 | Loss: 0.7420Epoch 25 | Loss: 0.7393 | AUC: 0.9085 | Acc: 90.94%Epoch 26 | Loss: 0.7313Epoch 27 | Loss: 0.7233Epoch 28 | Loss: 0.7207Epoch 29 | Loss: 0.7095Epoch 30 | Loss: 0.7336 | AUC: 0.9139 | Acc: 91.33%\n",
    "\n",
    "les 30 d'apres : 🚀 Lancement du Turbo Training sur 30 époques...\n",
    "Epoch 01 | Loss: 0.7269 | AUC: 0.9122 | Acc: 91.40%Epoch 02 | Loss: 0.7068 | AUC: 0.9177 | Acc: 91.48%Epoch 03 | Loss: 0.7130 | AUC: 0.9151 | Acc: 92.11%Epoch 04 | Loss: 0.7180 | AUC: 0.9136 | Acc: 91.47%Epoch 05 | Loss: 0.7161 | AUC: 0.9119 | Acc: 90.84%Epoch 06 | Loss: 0.7236 | AUC: 0.9094 | Acc: 90.64%Epoch 07 | Loss: 0.7326 | AUC: 0.9041 | Acc: 90.74%Epoch 08 | Loss: 0.7143 | AUC: 0.9107 | Acc: 91.18%Epoch 09 | Loss: 0.7265 | AUC: 0.9154 | Acc: 91.81%Epoch 10 | Loss: 0.6811 | AUC: 0.9190 | Acc: 91.63%Epoch 11 | Loss: 0.6718 | AUC: 0.9240 | Acc: 92.08%Epoch 12 | Loss: 0.6649 | AUC: 0.9328 | Acc: 92.87%Epoch 13 | Loss: 0.6412 | AUC: 0.9319 | Acc: 92.92%Epoch 14 | Loss: 0.6259 | AUC: 0.9331 | Acc: 92.91%Epoch 15 | Loss: 0.6095 | AUC: 0.9395 | Acc: 93.84%Epoch 16 | Loss: 0.5811 | AUC: 0.9420 | Acc: 93.72%Epoch 17 | Loss: 0.5711 | AUC: 0.9482 | Acc: 94.85%Epoch 18 | Loss: 0.5550 | AUC: 0.9467 | Acc: 94.05%Epoch 19 | Loss: 0.5472 | AUC: 0.9508 | Acc: 94.63%Epoch 20 | Loss: 0.5321 | AUC: 0.9555 | Acc: 95.34%Epoch 21 | Loss: 0.5284 | AUC: 0.9561 | Acc: 95.08%Epoch 22 | Loss: 0.4839 | AUC: 0.9572 | Acc: 95.41%Epoch 23 | Loss: 0.4849 | AUC: 0.9633 | Acc: 95.84%Epoch 24 | Loss: 0.4677 | AUC: 0.9590 | Acc: 95.66%Epoch 25 | Loss: 0.4680 | AUC: 0.9650 | Acc: 96.30%Epoch 26 | Loss: 0.4660 | AUC: 0.9646 | Acc: 95.89%Epoch 27 | Loss: 0.4720 | AUC: 0.9657 | Acc: 96.35%Epoch 28 | Loss: 0.4652 | AUC: 0.9630 | Acc: 95.64%Epoch 29 | Loss: 0.4406 | AUC: 0.9623 | Acc: 95.71%Epoch 30 | Loss: 0.4517 | AUC: 0.9630 | Acc: 96.11%\n",
    "\n",
    "les 30 encore autre :\n",
    "🚀 Lancement du Turbo Training sur 30 époques...\n",
    "Epoch 01 | Loss: 0.4526 | AUC: 0.9623 | Acc: 95.71%Epoch 02 | Loss: 0.4538 | AUC: 0.9645 | Acc: 95.71%Epoch 03 | Loss: 0.4668 | AUC: 0.9593 | Acc: 95.43%Epoch 04 | Loss: 0.4511 | AUC: 0.9641 | Acc: 95.86%Epoch 05 | Loss: 0.4859 | AUC: 0.9561 | Acc: 95.41%Epoch 06 | Loss: 0.5080 | AUC: 0.9539 | Acc: 94.76%Epoch 07 | Loss: 0.4996 | AUC: 0.9548 | Acc: 94.88%Epoch 08 | Loss: 0.4895 | AUC: 0.9530 | Acc: 94.95%Epoch 09 | Loss: 0.5143 | AUC: 0.9537 | Acc: 94.98%Epoch 10 | Loss: 0.4835 | AUC: 0.9563 | Acc: 95.10%Epoch 11 | Loss: 0.5041 | AUC: 0.9583 | Acc: 95.36%Epoch 12 | Loss: 0.4982 | AUC: 0.9613 | Acc: 95.69%Epoch 13 | Loss: 0.5004 | AUC: 0.9564 | Acc: 95.08%Epoch 14 | Loss: 0.4842 | AUC: 0.9617 | Acc: 95.68%Epoch 15 | Loss: 0.4721 | AUC: 0.9642 | Acc: 96.17%Epoch 16 | Loss: 0.4564 | AUC: 0.9649 | Acc: 96.09%Epoch 17 | Loss: 0.4398 | AUC: 0.9658 | Acc: 96.02%Epoch 18 | Loss: 0.4399 | AUC: 0.9685 | Acc: 96.64%Epoch 19 | Loss: 0.4422 | AUC: 0.9681 | Acc: 96.06%Epoch 20 | Loss: 0.4144 | AUC: 0.9682 | Acc: 96.52%Epoch 21 | Loss: 0.4029 | AUC: 0.9724 | Acc: 96.84%Epoch 22 | Loss: 0.3810 | AUC: 0.9716 | Acc: 96.79%Epoch 23 | Loss: 0.3750 | AUC: 0.9750 | Acc: 96.93%Epoch 24 | Loss: 0.3814 | AUC: 0.9755 | Acc: 97.33%Epoch 25 | Loss: 0.3674 | AUC: 0.9749 | Acc: 97.00%Epoch 26 | Loss: 0.3663 | AUC: 0.9783 | Acc: 97.60%Epoch 27 | Loss: 0.3552 | AUC: 0.9793 | Acc: 97.46%Epoch 28 | Loss: 0.3419 | AUC: 0.9778 | Acc: 97.23%Epoch 29 | Loss: 0.3450 | AUC: 0.9765 | Acc: 96.85%Epoch 30 | Loss: 0.3480 | AUC: 0.9786 | Acc: 97.46%\n",
    "\n",
    "les 30 encore encore autre : \n",
    "🚀 Lancement du Turbo Training sur 30 époques...Epoch 01 | Loss: 0.3380 | AUC: 0.9787 | Acc: 97.27%Epoch 02 | Loss: 0.3524 | AUC: 0.9786 | Acc: 97.65%Epoch 03 | Loss: 0.3638 | AUC: 0.9752 | Acc: 96.85%Epoch 04 | Loss: 0.3533 | AUC: 0.9753 | Acc: 97.27%Epoch 05 | Loss: 0.3854 | AUC: 0.9725 | Acc: 96.69%Epoch 06 | Loss: 0.3773 | AUC: 0.9709 | Acc: 96.88%Epoch 07 | Loss: 0.3999 | AUC: 0.9665 | Acc: 96.34%Epoch 08 | Loss: 0.4273 | AUC: 0.9688 | Acc: 96.34%Epoch 09 | Loss: 0.4237 | AUC: 0.9709 | Acc: 96.69%Epoch 10 | Loss: 0.4225 | AUC: 0.9678 | Acc: 96.69%Epoch 11 | Loss: 0.4079 | AUC: 0.9666 | Acc: 96.37%Epoch 12 | Loss: 0.4000 | AUC: 0.9693 | Acc: 96.79%Epoch 13 | Loss: 0.4027 | AUC: 0.9677 | Acc: 96.42%Epoch 14 | Loss: 0.3926 | AUC: 0.9729 | Acc: 96.93%Epoch 15 | Loss: 0.4037 | AUC: 0.9732 | Acc: 96.93%Epoch 16 | Loss: 0.3624 | AUC: 0.9725 | Acc: 96.93%Epoch 17 | Loss: 0.3618 | AUC: 0.9763 | Acc: 97.28%Epoch 18 | Loss: 0.3630 | AUC: 0.9759 | Acc: 97.08%Epoch 19 | Loss: 0.3447 | AUC: 0.9770 | Acc: 97.22%Epoch 20 | Loss: 0.3565 | AUC: 0.9787 | Acc: 97.70%Epoch 21 | Loss: 0.3365 | AUC: 0.9801 | Acc: 97.43%Epoch 22 | Loss: 0.3242 | AUC: 0.9807 | Acc: 97.66%Epoch 23 | Loss: 0.3190 | AUC: 0.9822 | Acc: 97.96%Epoch 24 | Loss: 0.3191 | AUC: 0.9805 | Acc: 98.01%Epoch 25 | Loss: 0.2910 | AUC: 0.9833 | Acc: 97.91%Epoch 26 | Loss: 0.2823 | AUC: 0.9848 | Acc: 98.26%Epoch 27 | Loss: 0.2786 | AUC: 0.9847 | Acc: 98.04%Epoch 28 | Loss: 0.2944 | AUC: 0.9838 | Acc: 98.14%Epoch 29 | Loss: 0.3035 | AUC: 0.9834 | Acc: 97.83%Epoch 30 | Loss: 0.2708 | AUC: 0.9847 | Acc: 98.28%\n",
    "\n",
    "les 30 autres :\n",
    "🚀 Lancement du Turbo Training sur 30 époques...\n",
    "Epoch 01 | Loss: 0.3010 | AUC: 0.9842 | Acc: 97.93%Epoch 02 | Loss: 0.2971 | AUC: 0.9845 | Acc: 98.38%Epoch 03 | Loss: 0.2812 | AUC: 0.9839 | Acc: 97.98%Epoch 04 | Loss: 0.3098 | AUC: 0.9821 | Acc: 97.75%Epoch 05 | Loss: 0.3209 | AUC: 0.9803 | Acc: 97.66%Epoch 06 | Loss: 0.3235 | AUC: 0.9793 | Acc: 97.60%Epoch 07 | Loss: 0.3481 | AUC: 0.9778 | Acc: 97.46%Epoch 08 | Loss: 0.3809 | AUC: 0.9746 | Acc: 96.88%Epoch 09 | Loss: 0.3576 | AUC: 0.9778 | Acc: 97.35%Epoch 10 | Loss: 0.3541 | AUC: 0.9767 | Acc: 97.27%Epoch 11 | Loss: 0.3862 | AUC: 0.9775 | Acc: 97.55%Epoch 12 | Loss: 0.3475 | AUC: 0.9772 | Acc: 97.61%Epoch 13 | Loss: 0.3290 | AUC: 0.9806 | Acc: 97.80%Epoch 14 | Loss: 0.3475 | AUC: 0.9809 | Acc: 97.95%Epoch 15 | Loss: 0.3305 | AUC: 0.9797 | Acc: 97.45%Epoch 16 | Loss: 0.3194 | AUC: 0.9807 | Acc: 97.73%Epoch 17 | Loss: 0.3344 | AUC: 0.9826 | Acc: 98.11%Epoch 18 | Loss: 0.3093 | AUC: 0.9842 | Acc: 98.31%Epoch 19 | Loss: 0.2861 | AUC: 0.9843 | Acc: 98.19%Epoch 20 | Loss: 0.2994 | AUC: 0.9845 | Acc: 98.39%Epoch 21 | Loss: 0.2685 | AUC: 0.9857 | Acc: 98.18%Epoch 22 | Loss: 0.2734 | AUC: 0.9857 | Acc: 98.24%Epoch 23 | Loss: 0.2692 | AUC: 0.9870 | Acc: 98.67%Epoch 24 | Loss: 0.2505 | AUC: 0.9880 | Acc: 98.61%Epoch 25 | Loss: 0.2499 | AUC: 0.9874 | Acc: 98.56%Epoch 26 | Loss: 0.2562 | AUC: 0.9872 | Acc: 98.39%Epoch 27 | Loss: 0.2489 | AUC: 0.9883 | Acc: 98.67%Epoch 28 | Loss: 0.2572 | AUC: 0.9864 | Acc: 98.51%Epoch 29 | Loss: 0.2518 | AUC: 0.9874 | Acc: 98.49%Epoch 30 | Loss: 0.2433 | AUC: 0.9872 | Acc: 98.34%\n",
    "\n",
    "les 30 encore autre \n",
    "🚀 Lancement du Turbo Training sur 30 époques...\n",
    "Epoch 01 | Loss: 0.2505 | AUC: 0.9877 | Acc: 98.49%Epoch 02 | Loss: 0.2356 | AUC: 0.9893 | Acc: 98.48%Epoch 03 | Loss: 0.2474 | AUC: 0.9883 | Acc: 98.67%Epoch 04 | Loss: 0.2593 | AUC: 0.9873 | Acc: 98.56%Epoch 05 | Loss: 0.2717 | AUC: 0.9862 | Acc: 98.33%Epoch 06 | Loss: 0.2830 | AUC: 0.9837 | Acc: 98.18%Epoch 07 | Loss: 0.3008 | AUC: 0.9841 | Acc: 98.33%Epoch 08 | Loss: 0.3088 | AUC: 0.9824 | Acc: 98.16%Epoch 09 | Loss: 0.3150 | AUC: 0.9823 | Acc: 98.31%Epoch 10 | Loss: 0.3397 | AUC: 0.9831 | Acc: 97.90%Epoch 11 | Loss: 0.3442 | AUC: 0.9820 | Acc: 98.09%Epoch 12 | Loss: 0.3263 | AUC: 0.9839 | Acc: 98.19%Epoch 13 | Loss: 0.3109 | AUC: 0.9820 | Acc: 98.00%Epoch 14 | Loss: 0.3066 | AUC: 0.9843 | Acc: 98.28%Epoch 15 | Loss: 0.2996 | AUC: 0.9823 | Acc: 98.01%Epoch 16 | Loss: 0.2941 | AUC: 0.9858 | Acc: 98.51%Epoch 17 | Loss: 0.2691 | AUC: 0.9843 | Acc: 98.06%Epoch 18 | Loss: 0.2702 | AUC: 0.9856 | Acc: 98.33%Epoch 19 | Loss: 0.2582 | AUC: 0.9858 | Acc: 98.43%Epoch 20 | Loss: 0.2696 | AUC: 0.9881 | Acc: 98.51%Epoch 21 | Loss: 0.2451 | AUC: 0.9872 | Acc: 98.49%Epoch 22 | Loss: 0.2335 | AUC: 0.9905 | Acc: 98.77%Epoch 23 | Loss: 0.2336 | AUC: 0.9899 | Acc: 98.82%Epoch 24 | Loss: 0.2404 | AUC: 0.9903 | Acc: 98.76%Epoch 25 | Loss: 0.2279 | AUC: 0.9911 | Acc: 98.89%Epoch 26 | Loss: 0.2132 | AUC: 0.9892 | Acc: 98.81%Epoch 27 | Loss: 0.2302 | AUC: 0.9900 | Acc: 98.87%Epoch 28 | Loss: 0.2249 | AUC: 0.9909 | Acc: 99.11%Epoch 29 | Loss: 0.2136 | AUC: 0.9908 | Acc: 98.87%Epoch 30 | Loss: 0.2143 | AUC: 0.9898 | Acc: 98.71%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5b000a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_data(df, min_len=3):\n",
    "    df = df.sort_values(by=['user_id', 'timestamp'])\n",
    "    user_groups = df.groupby('user_id')\n",
    "    \n",
    "    train_seqs = []\n",
    "    val_seqs = []\n",
    "    \n",
    "    for user_id, group in user_groups:\n",
    "        movie_ids = group['movie_id'].tolist()\n",
    "        if len(movie_ids) < min_len: continue\n",
    "            \n",
    "        # --- TRAIN : Historique jusqu'à avant-avant-dernier -> Cible : Avant-dernier ---\n",
    "        # Ex: [A, B, C, D, E] -> Input [A, B, C], Target D\n",
    "        train_seqs.append((movie_ids[:-2], movie_ids[-2]))\n",
    "        \n",
    "        # --- VAL : Historique jusqu'à avant-dernier -> Cible : Dernier ---\n",
    "        # Ex: [A, B, C, D, E] -> Input [A, B, C, D], Target E\n",
    "        # Attention : L'input de validation contient la cible d'entrainement !\n",
    "        val_seqs.append((movie_ids[:-1], movie_ids[-1]))\n",
    "\n",
    "    return train_seqs, val_seqs\n",
    "\n",
    "# Recréation des datasets\n",
    "train_data, val_data = get_train_val_data(df_reindexed) # Utilise ton df propre\n",
    "\n",
    "train_ds = SequentialDataset(train_data, num_items_seq, max_len=50)\n",
    "val_ds = SequentialDataset(val_data, num_items_seq, max_len=50)\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=128, shuffle=False, collate_fn=collate_fn) # Shuffle False pour Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8c713e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌟 SCORE RÉEL (Validation) : AUC = 0.7229 | Acc = 73.90%\n"
     ]
    }
   ],
   "source": [
    "# Fonction d'évaluation dédiée (Ne fait pas de backprop)\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    all_pos, all_neg = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            seq = batch['seq'].to(device)\n",
    "            pos = batch['pos'].to(device)\n",
    "            neg = batch['neg'].to(device)\n",
    "            \n",
    "            pos_logits, neg_logits = model(seq, pos, neg)\n",
    "            \n",
    "            all_pos.append(torch.sigmoid(pos_logits).cpu().numpy())\n",
    "            all_neg.append(torch.sigmoid(neg_logits).cpu().numpy())\n",
    "            \n",
    "    all_pos = np.concatenate(all_pos)\n",
    "    all_neg = np.concatenate(all_neg)\n",
    "    \n",
    "    acc = (all_pos > all_neg).mean()\n",
    "    auc = roc_auc_score(\n",
    "        np.concatenate([np.ones_like(all_pos), np.zeros_like(all_neg)]),\n",
    "        np.concatenate([all_pos, all_neg])\n",
    "    )\n",
    "    return auc, acc\n",
    "\n",
    "# TEST DE VERITE\n",
    "real_auc, real_acc = evaluate_model(model, val_loader)\n",
    "print(f\"🌟 SCORE RÉEL (Validation) : AUC = {real_auc:.4f} | Acc = {real_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9143a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SASRecSpectral(nn.Module):\n",
    "    # J'ai ajouté 'dropout' dans les arguments ici vvv\n",
    "    def __init__(self, num_users, num_items, max_len, d_model=64, num_heads=2, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        # Note : padding_idx=0 est crucial pour ignorer les 0 ajoutés par le pad_sequence\n",
    "        self.item_emb = nn.Embedding(num_items, d_model, padding_idx=0)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.dropout = nn.Dropout(dropout) # On utilise le dropout passé en argument\n",
    "        \n",
    "        # Blocs Séquentiels\n",
    "        self.blocks = nn.ModuleList([\n",
    "            # On passe le dropout à chaque bloc interne\n",
    "            SpectralTransformerBlock(d_model, num_heads, dropout=dropout, ortho_mode='soft')\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, item_seq):\n",
    "        # item_seq : [Batch, Seq_Len]\n",
    "        seq_len = item_seq.size(1)\n",
    "        \n",
    "        # --- Masque Causal (Empêche de voir le futur) ---\n",
    "        # Crée une matrice triangulaire : \n",
    "        # [0, -inf, -inf]\n",
    "        # [0, 0, -inf]\n",
    "        # [0, 0, 0]\n",
    "        # Tout ce qui est -inf sera ignoré par l'attention\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(item_seq.device)\n",
    "        \n",
    "        # Embeddings + Position\n",
    "        positions = torch.arange(seq_len, device=item_seq.device).unsqueeze(0)\n",
    "        \n",
    "        # Vérification de sécurité pour les positions (si seq_len > max_len)\n",
    "        if seq_len > self.pos_emb.num_embeddings:\n",
    "             positions = positions[:, :self.pos_emb.num_embeddings]\n",
    "             \n",
    "        x = self.item_emb(item_seq) + self.pos_emb(positions)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Passage dans les blocs\n",
    "        for block in self.blocks:\n",
    "            # On passe le masque à l'attention\n",
    "            x = block(x, mask=mask)\n",
    "            \n",
    "        x = self.final_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "66ca684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import copy\n",
    "\n",
    "# --- HYPERPARAMETRES (Mode Anti-Overfitting) ---\n",
    "EPOCHS = 50\n",
    "LR = 1e-3              # Plus lent pour mieux apprendre (avant: 1e-3)\n",
    "WEIGHT_DECAY = 1e-4    # Pénalité pour empêcher les poids d'exploser (avant: 1e-5)\n",
    "DROPOUT = 0.1          # On éteint 30% des neurones aléatoirement (avant: 0.1)\n",
    "\n",
    "# --- RE-INITIALISATION DU MODELE (Pour appliquer le Dropout) ---\n",
    "# On recrée le modèle pour être sûr d'avoir les bons paramètres\n",
    "base_model = SASRecSpectral(\n",
    "    num_users=num_users, \n",
    "    num_items=num_items_seq, \n",
    "    max_len=50, \n",
    "    d_model=128, \n",
    "    num_layers=4,\n",
    "    dropout=DROPOUT # <--- Important !\n",
    ").to(device)\n",
    "\n",
    "# Padding Index\n",
    "base_model.item_emb = nn.Embedding(num_items_seq, 128, padding_idx=0).to(device)\n",
    "\n",
    "# Le Wrapper avec Tête Spectrale\n",
    "model = SASRecWithSpectralHead(base_model, d_model=128).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, \n",
    "    max_lr=LR, \n",
    "    steps_per_epoch=len(train_loader), \n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dc49fd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Lancement de l'entraînement (Validation stricte)...\n",
      "---------------------------------------------------------------------------\n",
      "Epoch |  Train Loss  |  VAL AUC   |  VAL ACC   |   Status  \n",
      "---------------------------------------------------------------------------\n",
      "01/50 | 2.1556       | 0.5491     | 55.41%    | 🔥 BEST\n",
      "02/50 | 1.7447       | 0.5684     | 57.25%    | 🔥 BEST\n",
      "03/50 | 1.6216       | 0.5751     | 57.05%    | 🔥 BEST\n",
      "04/50 | 1.5536       | 0.5932     | 58.97%    | 🔥 BEST\n",
      "05/50 | 1.5054       | 0.6105     | 60.76%    | 🔥 BEST\n",
      "06/50 | 1.4687       | 0.6186     | 62.29%    | 🔥 BEST\n",
      "07/50 | 1.4285       | 0.6233     | 62.58%    | 🔥 BEST\n",
      "08/50 | 1.4195       | 0.6328     | 63.10%    | 🔥 BEST\n",
      "09/50 | 1.3576       | 0.6408     | 64.52%    | 🔥 BEST\n",
      "10/50 | 1.3392       | 0.6548     | 65.48%    | 🔥 BEST\n",
      "11/50 | 1.3337       | 0.6608     | 66.15%    | 🔥 BEST\n",
      "12/50 | 1.3086       | 0.6689     | 66.46%    | 🔥 BEST\n",
      "13/50 | 1.2932       | 0.6749     | 67.59%    | 🔥 BEST\n",
      "14/50 | 1.2709       | 0.6927     | 69.43%    | 🔥 BEST\n",
      "15/50 | 1.2638       | 0.6860     | 68.58%    | \n",
      "16/50 | 1.2389       | 0.6981     | 69.56%    | 🔥 BEST\n",
      "17/50 | 1.2337       | 0.7048     | 70.80%    | 🔥 BEST\n",
      "18/50 | 1.2201       | 0.7042     | 69.94%    | \n",
      "19/50 | 1.2043       | 0.7145     | 70.99%    | 🔥 BEST\n",
      "20/50 | 1.1831       | 0.7185     | 71.90%    | 🔥 BEST\n",
      "21/50 | 1.1765       | 0.7249     | 72.69%    | 🔥 BEST\n",
      "22/50 | 1.1686       | 0.7195     | 71.35%    | \n",
      "23/50 | 1.1642       | 0.7277     | 72.96%    | 🔥 BEST\n",
      "24/50 | 1.1555       | 0.7292     | 73.37%    | 🔥 BEST\n",
      "25/50 | 1.1409       | 0.7354     | 73.19%    | 🔥 BEST\n",
      "26/50 | 1.1313       | 0.7337     | 73.24%    | \n",
      "27/50 | 1.1249       | 0.7401     | 74.32%    | 🔥 BEST\n",
      "28/50 | 1.1165       | 0.7415     | 74.27%    | 🔥 BEST\n",
      "29/50 | 1.1023       | 0.7465     | 74.58%    | 🔥 BEST\n",
      "30/50 | 1.1033       | 0.7441     | 74.40%    | \n",
      "31/50 | 1.0854       | 0.7543     | 74.93%    | 🔥 BEST\n",
      "32/50 | 1.0765       | 0.7492     | 74.78%    | \n",
      "33/50 | 1.0645       | 0.7538     | 75.79%    | \n",
      "34/50 | 1.0624       | 0.7553     | 75.68%    | 🔥 BEST\n",
      "35/50 | 1.0608       | 0.7525     | 75.64%    | \n",
      "36/50 | 1.0462       | 0.7542     | 75.96%    | \n",
      "37/50 | 1.0474       | 0.7566     | 74.85%    | 🔥 BEST\n",
      "38/50 | 1.0512       | 0.7598     | 75.51%    | 🔥 BEST\n",
      "39/50 | 1.0311       | 0.7681     | 77.33%    | 🔥 BEST\n",
      "40/50 | 1.0166       | 0.7549     | 75.87%    | \n",
      "41/50 | 1.0224       | 0.7629     | 76.32%    | \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 70\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m75\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# 1. Train\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# 2. Validation (La vérité)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     val_auc, val_acc \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader)\n",
      "Cell \u001b[0;32mIn[112], line 13\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     10\u001b[0m neg \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m pos_logits, neg_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Loss sur Positifs (Target 1) et Négatifs (Target 0)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pos_logits, torch\u001b[38;5;241m.\u001b[39mones_like(pos_logits)) \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m     17\u001b[0m        criterion(neg_logits, torch\u001b[38;5;241m.\u001b[39mzeros_like(neg_logits))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[78], line 18\u001b[0m, in \u001b[0;36mSASRecWithSpectralHead.forward\u001b[0;34m(self, seq, pos_items, neg_items)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, seq, pos_items, neg_items):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# 1. Extraction du Contexte (Via le Transformer)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# [Batch, Seq_Len, Dim]\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     log_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# On prend le dernier état (le résumé de l'historique)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     user_context \u001b[38;5;241m=\u001b[39m log_feats[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;66;03m# [Batch, Dim]\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[95], line 49\u001b[0m, in \u001b[0;36mSASRecSpectral.forward\u001b[0;34m(self, item_seq)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Passage dans les blocs\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# On passe le masque à l'attention\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_norm(x)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[60], line 32\u001b[0m, in \u001b[0;36mSpectralTransformerBlock.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(attn_out))\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# 2. Spectral Bilinear Network (Gère le raisonnement)\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m spectral_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspectral_ffn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(spectral_out))\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/spectral-neural-network-SNN/DSN.py:378\u001b[0m, in \u001b[0;36mMultiBasisSpectralLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    374\u001b[0m combined_energy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(all_energies, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# 3. Combinaison linéaire (Valeurs propres)\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# y = Sum( lambda_ijk * energy_jk )\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meigen_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_energy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- FONCTIONS UTILITAIRES ---\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        seq = batch['seq'].to(device)\n",
    "        pos = batch['pos'].to(device)\n",
    "        neg = batch['neg'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pos_logits, neg_logits = model(seq, pos, neg)\n",
    "        \n",
    "        # Loss sur Positifs (Target 1) et Négatifs (Target 0)\n",
    "        loss = criterion(pos_logits, torch.ones_like(pos_logits)) + \\\n",
    "               criterion(neg_logits, torch.zeros_like(neg_logits))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_pos, all_neg = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            seq = batch['seq'].to(device)\n",
    "            pos = batch['pos'].to(device)\n",
    "            neg = batch['neg'].to(device)\n",
    "            \n",
    "            pos_logits, neg_logits = model(seq, pos, neg)\n",
    "            \n",
    "            all_pos.append(torch.sigmoid(pos_logits).cpu().numpy())\n",
    "            all_neg.append(torch.sigmoid(neg_logits).cpu().numpy())\n",
    "            \n",
    "    # Concaténation\n",
    "    if len(all_pos) == 0: return 0, 0 # Sécurité si loader vide\n",
    "    \n",
    "    all_pos = np.concatenate(all_pos)\n",
    "    all_neg = np.concatenate(all_neg)\n",
    "    \n",
    "    # Métriques\n",
    "    accuracy = (all_pos > all_neg).mean()\n",
    "    \n",
    "    y_true = np.concatenate([np.ones_like(all_pos), np.zeros_like(all_neg)])\n",
    "    y_scores = np.concatenate([all_pos, all_neg])\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "    \n",
    "    return auc, accuracy\n",
    "\n",
    "# --- BOUCLE PRINCIPALE ---\n",
    "\n",
    "best_auc = 0.0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "patience = 10\n",
    "no_improve = 0\n",
    "\n",
    "print(f\"🚀 Lancement de l'entraînement (Validation stricte)...\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'Epoch':^5} | {'Train Loss':^12} | {'VAL AUC':^10} | {'VAL ACC':^10} | {'Status':^10}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # 1. Train\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    \n",
    "    # 2. Validation (La vérité)\n",
    "    val_auc, val_acc = evaluate(model, val_loader)\n",
    "    \n",
    "    # 3. Checkpoint & Affichage\n",
    "    is_best = val_auc > best_auc\n",
    "    status = \"🔥 BEST\" if is_best else \"\"\n",
    "    \n",
    "    print(f\"{epoch+1:02d}/{EPOCHS} | {train_loss:.4f}       | {val_auc:.4f}     | {val_acc*100:.2f}%    | {status}\")\n",
    "    \n",
    "    if is_best:\n",
    "        best_auc = val_auc\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        no_improve = 0\n",
    "        # Sauvegarde disque immédiate\n",
    "        torch.save(model.state_dict(), \"best_sasrec_spectral.pth\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        \n",
    "    # Early Stopping\n",
    "    if no_improve >= patience:\n",
    "        print(f\"\\n⏹️ Arrêt précoce : Pas d'amélioration depuis {patience} époques.\")\n",
    "        break\n",
    "\n",
    "print(\"-\" * 75)\n",
    "print(f\"🏆 Meilleur Résultat Final -> Val AUC: {best_auc:.4f}\")\n",
    "\n",
    "# On recharge les meilleurs poids pour finir\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2a71bdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Lancement Config 'Sanity Check' (LR=0.01, Dim=64)...\n",
      "---------------------------------------------------------------------------\n",
      "Epoch |  Train Loss  |  VAL AUC   |  VAL ACC   |   Status  \n",
      "---------------------------------------------------------------------------\n",
      "01/50 | 4.4262       | 0.6050     | 60.31%    | 🔥 BEST\n",
      "02/50 | 1.8460       | 0.6453     | 64.49%    | 🔥 BEST\n",
      "03/50 | 1.4305       | 0.6993     | 70.01%    | 🔥 BEST\n",
      "04/50 | 1.2762       | 0.7321     | 73.92%    | 🔥 BEST\n",
      "05/50 | 1.1617       | 0.7598     | 76.16%    | 🔥 BEST\n",
      "06/50 | 1.0690       | 0.7665     | 76.57%    | 🔥 BEST\n",
      "07/50 | 1.0434       | 0.7734     | 77.51%    | 🔥 BEST\n",
      "08/50 | 0.9770       | 0.7744     | 78.11%    | 🔥 BEST\n",
      "09/50 | 0.9400       | 0.7770     | 77.32%    | 🔥 BEST\n",
      "10/50 | 0.9354       | 0.7811     | 77.71%    | 🔥 BEST\n",
      "11/50 | 0.9122       | 0.7806     | 77.90%    | \n",
      "12/50 | 0.9160       | 0.7845     | 78.86%    | 🔥 BEST\n",
      "13/50 | 0.9269       | 0.7802     | 77.71%    | \n",
      "14/50 | 0.9256       | 0.7829     | 78.19%    | \n",
      "15/50 | 0.8977       | 0.7810     | 77.73%    | \n",
      "16/50 | 0.9069       | 0.7781     | 77.88%    | \n",
      "17/50 | 0.8917       | 0.7826     | 78.39%    | \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 42\u001b[0m\n\u001b[1;32m     38\u001b[0m patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     val_auc, val_acc \u001b[38;5;241m=\u001b[39m evaluate_model(model, val_loader)\n",
      "Cell \u001b[0;32mIn[112], line 19\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Loss sur Positifs (Target 1) et Négatifs (Target 0)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pos_logits, torch\u001b[38;5;241m.\u001b[39mones_like(pos_logits)) \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m     17\u001b[0m        criterion(neg_logits, torch\u001b[38;5;241m.\u001b[39mzeros_like(neg_logits))\n\u001b[0;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- 1. CONFIGURATION \"GOLD STANDARD\" ---\n",
    "EPOCHS = 50\n",
    "LR = 1e-2              # C'est LA clé. Pas 1e-2.\n",
    "WEIGHT_DECAY = 1e-5    # Légère régularisation\n",
    "DROPOUT = 0.2          # Suffisant pour 64 dim\n",
    "\n",
    "# --- 2. MODELE PLUS LEGER & AGILE ---\n",
    "# On revient à 2 couches / 64 dim.\n",
    "# Pourquoi ? Parce que si ça marche pas à 64, ça marchera pas à 128.\n",
    "base_model = SASRecSpectral(\n",
    "    num_users=num_users, \n",
    "    num_items=num_items_seq, \n",
    "    max_len=50, \n",
    "    d_model=64,       \n",
    "    num_layers=2,     \n",
    "    dropout=DROPOUT \n",
    ").to(device)\n",
    "\n",
    "# Padding Index\n",
    "base_model.item_emb = nn.Embedding(num_items_seq, 64, padding_idx=0).to(device)\n",
    "\n",
    "# --- 3. TÊTE SIMPLE (Dot Product) ---\n",
    "# On utilise le wrapper simple. C'est la méthode éprouvée pour SASRec.\n",
    "# Ton innovation est DANS le backbone (SpectralTransformerBlock), pas besoin d'en rajouter dans la tête.\n",
    "model = SASRecWrapper(base_model).to(device)\n",
    "\n",
    "# Optimiseur\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# --- 4. LANCEMENT ---\n",
    "print(f\"🚀 Lancement Config 'Sanity Check' (LR={LR}, Dim=64)...\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'Epoch':^5} | {'Train Loss':^12} | {'VAL AUC':^10} | {'VAL ACC':^10} | {'Status':^10}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "best_auc = 0.0\n",
    "patience = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    \n",
    "    # Validation\n",
    "    val_auc, val_acc = evaluate_model(model, val_loader)\n",
    "    \n",
    "    # Logique Best\n",
    "    is_best = val_auc > best_auc\n",
    "    status = \"🔥 BEST\" if is_best else \"\"\n",
    "    \n",
    "    print(f\"{epoch+1:02d}/{EPOCHS} | {train_loss:.4f}       | {val_auc:.4f}     | {val_acc*100:.2f}%    | {status}\")\n",
    "    \n",
    "    if is_best:\n",
    "        best_auc = val_auc\n",
    "        patience = 0\n",
    "        torch.save(model.state_dict(), \"best_sasrec_spectral_final.pth\")\n",
    "    else:\n",
    "        patience += 1\n",
    "        \n",
    "    if patience > 10:\n",
    "        print(\"Stop: Pas d'amélioration.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c1ccd869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/50 | 0.8950       | 0.7880     | 78.31%    | 🔥 BEST\n",
      "02/50 | 0.8621       | 0.7821     | 78.66%    | \n",
      "03/50 | 0.8733       | 0.7883     | 78.72%    | 🔥 BEST\n",
      "04/50 | 0.8730       | 0.7847     | 78.91%    | \n",
      "05/50 | 0.8535       | 0.7813     | 78.21%    | \n",
      "06/50 | 0.8680       | 0.7849     | 78.61%    | \n",
      "07/50 | 0.8574       | 0.7893     | 78.26%    | 🔥 BEST\n",
      "08/50 | 0.8483       | 0.7916     | 79.75%    | 🔥 BEST\n",
      "09/50 | 0.8389       | 0.7900     | 78.99%    | \n",
      "10/50 | 0.8580       | 0.7897     | 79.04%    | \n",
      "11/50 | 0.8778       | 0.7882     | 78.89%    | \n",
      "12/50 | 0.8430       | 0.7867     | 78.53%    | \n",
      "13/50 | 0.8510       | 0.7878     | 78.72%    | \n",
      "14/50 | 0.8421       | 0.7867     | 78.82%    | \n",
      "15/50 | 0.8488       | 0.7868     | 78.44%    | \n",
      "16/50 | 0.8372       | 0.7899     | 78.09%    | \n",
      "17/50 | 0.8429       | 0.7884     | 78.38%    | \n",
      "18/50 | 0.8506       | 0.7913     | 79.12%    | \n",
      "19/50 | 0.8525       | 0.7915     | 79.24%    | \n",
      "Stop: Pas d'amélioration.\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=WEIGHT_DECAY)\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    \n",
    "    # Validation\n",
    "    val_auc, val_acc = evaluate_model(model, val_loader)\n",
    "    \n",
    "    # Logique Best\n",
    "    is_best = val_auc > best_auc\n",
    "    status = \"🔥 BEST\" if is_best else \"\"\n",
    "    \n",
    "    print(f\"{epoch+1:02d}/{EPOCHS} | {train_loss:.4f}       | {val_auc:.4f}     | {val_acc*100:.2f}%    | {status}\")\n",
    "    \n",
    "    if is_best:\n",
    "        best_auc = val_auc\n",
    "        patience = 0\n",
    "        torch.save(model.state_dict(), \"best_sasrec_spectral_final.pth\")\n",
    "    else:\n",
    "        patience += 1\n",
    "        \n",
    "    if patience > 10:\n",
    "        print(\"Stop: Pas d'amélioration.\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
