{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92f3bbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "\n",
    "# Config Hardware\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Working on {device}\")\n",
    "\n",
    "def load_criteo_sample(filepath='data/criteo/train.txt', num_rows=1000000):\n",
    "    print(f\"‚è≥ Chargement de {num_rows} lignes depuis {filepath}...\")\n",
    "    \n",
    "    cols = ['label'] + [f'I{i}' for i in range(1, 14)] + [f'C{i}' for i in range(1, 27)]\n",
    "    \n",
    "    # Lecture (on pr√©cise header=None car le fichier n'a pas d'en-t√™te)\n",
    "    df = pd.read_csv(filepath, sep='\\t', names=cols, nrows=num_rows, header=None)\n",
    "    \n",
    "    # --- PREPROCESSING NUMERIQUE (BLIND√â) ---\n",
    "    print(\"‚öôÔ∏è Nettoyage Num√©rique (Force Numeric + Log + Scaling)...\")\n",
    "    \n",
    "    # 1. Force la conversion en nombres (transforme les erreurs/strings en NaN)\n",
    "    # C'est l'√©tape qui manquait : elle nettoie les donn√©es corrompues\n",
    "    for col in [f'I{i}' for i in range(1, 14)]:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # 2. R√©cup√©ration des valeurs + Remplissage des trous\n",
    "    x_num = df[[f'I{i}' for i in range(1, 14)]].fillna(0).values.astype(np.float32)\n",
    "    \n",
    "    # 3. Log transformation (log1p est plus s√ªr que log)\n",
    "    x_num = np.log1p(x_num)\n",
    "    \n",
    "    # 4. SAFETY CHECK : On remplace les Infinis √©ventuels par 0\n",
    "    x_num = np.nan_to_num(x_num)\n",
    "    \n",
    "    # 5. Scaling\n",
    "    scaler = StandardScaler()\n",
    "    x_num = scaler.fit_transform(x_num)\n",
    "    \n",
    "    # --- PREPROCESSING CATEGORIEL ---\n",
    "    print(\"‚öôÔ∏è Nettoyage Cat√©goriel (Hashing Trick)...\")\n",
    "    x_cat = np.zeros((len(df), 26), dtype=np.int64)\n",
    "    VOCAB_SIZE = 20000 \n",
    "    \n",
    "    for i, col in enumerate([f'C{i}' for i in range(1, 27)]):\n",
    "        # On convertit tout en string avant de hasher pour √©viter les bugs de type\n",
    "        df[col] = df[col].fillna(\"missing\").astype(str).apply(lambda x: hash(x) % VOCAB_SIZE)\n",
    "        x_cat[:, i] = df[col].values\n",
    "        \n",
    "    y = df['label'].values.astype(np.float32)\n",
    "    \n",
    "    print(f\"‚úÖ Donn√©es pr√™tes et propres : {len(y)} √©chantillons.\")\n",
    "    return x_num, x_cat, y, VOCAB_SIZE\n",
    "\n",
    "# Dataset PyTorch\n",
    "class CriteoDataset(Dataset):\n",
    "    def __init__(self, x_num, x_cat, y):\n",
    "        self.x_num = torch.tensor(x_num, dtype=torch.float32)\n",
    "        self.x_cat = torch.tensor(x_cat, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx): return self.x_num[idx], self.x_cat[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d63eed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- A. TON DSN (Spectral Interaction) ---\n",
    "from SBN import MultiBasisBilinearLayer\n",
    "\n",
    "class CriteoDSN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=16):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        # Input Size = 13 (Num) + 26 * Embed_Dim (Cat)\n",
    "        self.input_dim = 13 + (26 * embed_dim)\n",
    "        \n",
    "        # Le Cerveau Spectral\n",
    "        self.dsn = MultiBasisBilinearLayer(self.input_dim, 1, num_bases=1)\n",
    "        \n",
    "    def forward(self, x_num, x_cat):\n",
    "        # Flatten des embeddings : [Batch, 26 * 16]\n",
    "        embs = self.emb(x_cat).view(x_cat.size(0), -1)\n",
    "        # Concat√©nation Totale\n",
    "        x = torch.cat([x_num, embs], dim=1)\n",
    "        return self.dsn(x).squeeze(-1)\n",
    "\n",
    "# --- B. LE MLP (Baseline Standard) ---\n",
    "class CriteoMLP(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=16):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.input_dim = 13 + (26 * embed_dim)\n",
    "        \n",
    "        # Tour MLP Classique (ReLU)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_num, x_cat):\n",
    "        embs = self.emb(x_cat).view(x_cat.size(0), -1)\n",
    "        x = torch.cat([x_num, embs], dim=1)\n",
    "        return self.mlp(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acaeab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "def train_eval(model, train_loader, val_loader, name=\"Model\"):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    print(f\"\\nü•ä D√©marrage Entra√Ænement : {name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for epoch in range(10): \n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_grad_norm = 0.0 # Variable pour cumuler les normes\n",
    "        \n",
    "        for x_n, x_c, y in train_loader:\n",
    "            x_n, x_c, y = x_n.to(device), x_c.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x_n, x_c)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # --- CALCUL DE LA NORME DU GRADIENT ---\n",
    "            # On calcule la norme L2 de tous les gradients concat√©n√©s\n",
    "            batch_norm = 0.0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.detach().data.norm(2)\n",
    "                    batch_norm += param_norm.item() ** 2\n",
    "            batch_norm = batch_norm ** 0.5\n",
    "            \n",
    "            total_grad_norm += batch_norm\n",
    "            # --------------------------------------\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        # Moyennes pour l'√©poque\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_grad_norm = total_grad_norm / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        all_preds, all_y = [], []\n",
    "        with torch.no_grad():\n",
    "            for x_n, x_c, y in val_loader:\n",
    "                x_n, x_c, y = x_n.to(device), x_c.to(device), y.to(device)\n",
    "                logits = model(x_n, x_c)\n",
    "                all_preds.append(torch.sigmoid(logits).cpu().numpy())\n",
    "                all_y.append(y.cpu().numpy())\n",
    "                \n",
    "        auc = roc_auc_score(np.concatenate(all_y), np.concatenate(all_preds))\n",
    "        dt = time.time() - t0\n",
    "        \n",
    "        # Affichage avec la Norme du Gradient\n",
    "        print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | VAL AUC: {auc:.4f} | Grad Norm: {avg_grad_norm:.4f} | Time: {dt:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bba5c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Chargement de 1000000 lignes depuis data/criteo/train.txt...\n",
      "‚öôÔ∏è Nettoyage Num√©rique (Force Numeric + Log + Scaling)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qy/26d_rwv5383c0fvysb2ywjl40000gn/T/ipykernel_70002/3736831135.py:34: RuntimeWarning: divide by zero encountered in log1p\n",
      "  x_num = np.log1p(x_num)\n",
      "/var/folders/qy/26d_rwv5383c0fvysb2ywjl40000gn/T/ipykernel_70002/3736831135.py:34: RuntimeWarning: invalid value encountered in log1p\n",
      "  x_num = np.log1p(x_num)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Nettoyage Cat√©goriel (Hashing Trick)...\n",
      "‚úÖ Donn√©es pr√™tes et propres : 1000000 √©chantillons.\n"
     ]
    }
   ],
   "source": [
    "# --- EXECUTION DU DUEL ---\n",
    "\n",
    "# 1. Chargement (Adapte le chemin 'train.txt' si besoin)\n",
    "x_num, x_cat, y, vocab_size = load_criteo_sample('data/criteo/train.txt', num_rows=1000000)\n",
    "\n",
    "# 2. Split Train/Val (80/20)\n",
    "split = int(0.8 * len(y))\n",
    "train_ds = CriteoDataset(x_num[:split], x_cat[:split], y[:split])\n",
    "val_ds = CriteoDataset(x_num[split:], x_cat[split:], y[split:])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=1024, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5483a4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü•ä D√©marrage Entra√Ænement : Baseline MLP\n",
      "--------------------------------------------------\n",
      "Epoch 1 | Loss: 0.4933 | VAL AUC: 0.7610 | Grad Norm: 0.2130 | Time: 9.4s\n",
      "Epoch 2 | Loss: 0.4767 | VAL AUC: 0.7678 | Grad Norm: 0.1615 | Time: 10.0s\n",
      "Epoch 3 | Loss: 0.4682 | VAL AUC: 0.7718 | Grad Norm: 0.1494 | Time: 13.7s\n",
      "Epoch 4 | Loss: 0.4615 | VAL AUC: 0.7737 | Grad Norm: 0.1423 | Time: 11.4s\n",
      "Epoch 5 | Loss: 0.4554 | VAL AUC: 0.7743 | Grad Norm: 0.1380 | Time: 9.7s\n",
      "Epoch 6 | Loss: 0.4497 | VAL AUC: 0.7760 | Grad Norm: 0.1366 | Time: 10.1s\n",
      "Epoch 7 | Loss: 0.4437 | VAL AUC: 0.7756 | Grad Norm: 0.1379 | Time: 8.7s\n",
      "Epoch 8 | Loss: 0.4385 | VAL AUC: 0.7742 | Grad Norm: 0.1393 | Time: 8.8s\n",
      "Epoch 9 | Loss: 0.4335 | VAL AUC: 0.7737 | Grad Norm: 0.1427 | Time: 8.9s\n",
      "Epoch 10 | Loss: 0.4277 | VAL AUC: 0.7713 | Grad Norm: 0.1461 | Time: 8.7s\n",
      "\n",
      "ü•ä D√©marrage Entra√Ænement : Ton DSN (Spectral)\n",
      "--------------------------------------------------\n",
      "Epoch 1 | Loss: 0.4948 | VAL AUC: 0.7595 | Grad Norm: 0.3444 | Time: 37.2s\n",
      "Epoch 2 | Loss: 0.4701 | VAL AUC: 0.7621 | Grad Norm: 0.2916 | Time: 40.3s\n",
      "Epoch 3 | Loss: 0.4623 | VAL AUC: 0.7642 | Grad Norm: 0.2775 | Time: 43.1s\n",
      "Epoch 4 | Loss: 0.4557 | VAL AUC: 0.7649 | Grad Norm: 0.2676 | Time: 44.6s\n",
      "Epoch 5 | Loss: 0.4501 | VAL AUC: 0.7648 | Grad Norm: 0.2590 | Time: 44.1s\n",
      "Epoch 6 | Loss: 0.4445 | VAL AUC: 0.7656 | Grad Norm: 0.2536 | Time: 44.3s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m mlp \u001b[38;5;241m=\u001b[39m CriteoMLP(vocab_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m train_eval(mlp, train_loader, val_loader, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseline MLP\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrain_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTon DSN (Spectral)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[50], line 25\u001b[0m, in \u001b[0;36mtrain_eval\u001b[0;34m(model, train_loader, val_loader, name)\u001b[0m\n\u001b[1;32m     23\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(x_n, x_c)\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, y)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# --- CALCUL DE LA NORME DU GRADIENT ---\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# On calcule la norme L2 de tous les gradients concat√©n√©s\u001b[39;00m\n\u001b[1;32m     29\u001b[0m batch_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3. FIGHT !\n",
    "dsn = CriteoDSN(vocab_size).to(device)\n",
    "mlp = CriteoMLP(vocab_size).to(device)\n",
    "\n",
    "train_eval(mlp, train_loader, val_loader, name=\"Baseline MLP\")\n",
    "train_eval(dsn, train_loader, val_loader, name=\"Ton DSN (Spectral)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feda434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "# --- 1. COUCHE SPECTRALE VECTORIS√âE (Optimis√©e Vitesse) ---\n",
    "class VectorizedSpectralLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_bases=2): # Bases r√©duites √† 2 pour la vitesse\n",
    "        super().__init__()\n",
    "        self.num_bases = num_bases\n",
    "        \n",
    "        # On utilise une seule matrice de poids [Bases, In, In] pour √©viter les boucles Python lentes\n",
    "        # Mais pour rester simple et compatible, on garde les Linear mais on r√©duit la dimension\n",
    "        self.right_bases = nn.ModuleList([nn.Linear(in_features, 32) for _ in range(num_bases)]) # Projection Low-Rank\n",
    "        self.left_bases = nn.ModuleList([nn.Linear(in_features, 32) for _ in range(num_bases)])\n",
    "        \n",
    "        self.eigen_weights = nn.Linear(32 * num_bases, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [Batch, In_Features]\n",
    "        \n",
    "        all_interactions = []\n",
    "        for i in range(self.num_bases):\n",
    "            # Interaction dans un espace r√©duit (32 dim) pour aller tr√®s vite\n",
    "            r = self.right_bases[i](x)\n",
    "            l = self.left_bases[i](x)\n",
    "            all_interactions.append(r * l) # Interaction Bilin√©aire\n",
    "            \n",
    "        # [Batch, 32 * Num_Bases]\n",
    "        combined = torch.cat(all_interactions, dim=-1)\n",
    "        return self.eigen_weights(combined)\n",
    "\n",
    "# --- 2. ARCHITECTURE \"SPECTRAL CROSS\" ---\n",
    "class CriteoSpectralCross(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=16):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.num_proj = nn.Linear(13, 13 * embed_dim)\n",
    "        \n",
    "        # 39 champs * 16 dim = 624 dimensions\n",
    "        self.total_dim = (13 + 26) * embed_dim\n",
    "        \n",
    "        # --- L'INNOVATION ---\n",
    "        # Au lieu d'un simple Linear(624, 256), on met ton Spectral Layer\n",
    "        # Il force le croisement des donn√©es AVANT de r√©duire la dimension\n",
    "        self.spectral_layer = VectorizedSpectralLayer(self.total_dim, 256, num_bases=2)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        # Finition MLP classique pour la d√©cision\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_num, x_cat):\n",
    "        batch_size = x_num.size(0)\n",
    "        \n",
    "        # 1. Prepare Inputs\n",
    "        num_vecs = self.num_proj(x_num).view(batch_size, -1) # [Batch, 13*16]\n",
    "        cat_vecs = self.emb(x_cat).view(batch_size, -1)      # [Batch, 26*16]\n",
    "        \n",
    "        # 2. Global Concatenation (Comme le MLP)\n",
    "        # C'est crucial pour la vitesse : on ne fait qu'une seule grosse op√©ration\n",
    "        x = torch.cat([num_vecs, cat_vecs], dim=1) # [Batch, 624]\n",
    "        \n",
    "        # 3. Spectral Interaction & Compression\n",
    "        x = self.spectral_layer(x) # [Batch, 256]\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        # 4. D√©cision\n",
    "        return self.final_mlp(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8163a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "def train_eval(model, train_loader, val_loader, name=\"Model\"):\n",
    "    # CHANGEMENT 1 : Weight Decay ajust√© √† 1e-3 (Le juste milieu pour stabiliser sans tuer)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-3)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    print(f\"\\nü•ä D√©marrage Entra√Ænement : {name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for epoch in range(10): \n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_grad_norm = 0.0\n",
    "        \n",
    "        for x_n, x_c, y in train_loader:\n",
    "            x_n, x_c, y = x_n.to(device), x_c.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logits = model(x_n, x_c)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # CHANGEMENT 2 : Clipping strict √† 0.2\n",
    "            # On force le DSN √† √™tre aussi stable que le MLP (qui est naturellement √† ~0.20)\n",
    "            norm_tensor = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.2)\n",
    "            \n",
    "            total_grad_norm += norm_tensor.item()\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        # Moyennes\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_grad_norm = total_grad_norm / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        all_preds, all_y = [], []\n",
    "        with torch.no_grad():\n",
    "            for x_n, x_c, y in val_loader:\n",
    "                x_n, x_c, y = x_n.to(device), x_c.to(device), y.to(device)\n",
    "                logits = model(x_n, x_c)\n",
    "                probs = torch.sigmoid(logits)\n",
    "                all_preds.append(probs.cpu().numpy())\n",
    "                all_y.append(y.cpu().numpy())\n",
    "                \n",
    "        auc = roc_auc_score(np.concatenate(all_y), np.concatenate(all_preds))\n",
    "        dt = time.time() - t0\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | VAL AUC: {auc:.4f} | Grad Norm: {avg_grad_norm:.4f} | Time: {dt:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f17d9cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from SBN2 import DeepMultiBasisBilinearNet  # <--- On utilise TON fichier\n",
    "\n",
    "class CriteoDeepWrapper(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=16, num_bases=4, rank_factor=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- 1. Pr√©paration des Features (Standard) ---\n",
    "        self.emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.num_proj = nn.Linear(13, 13 * embed_dim)\n",
    "\n",
    "        # Dimension totale = (13 num√©riques + 26 cat√©gorielles) * 16\n",
    "        input_dim = (13 + 26) * embed_dim  # 624\n",
    "\n",
    "        # --- 2. TON BACKBONE (DeepMultiBasisBilinearNet) ---\n",
    "        # Architecture : 624 -> 256 -> 128 -> 1\n",
    "        # rank_factor contr√¥le la compression Low-Rank interne\n",
    "        self.backbone = DeepMultiBasisBilinearNet(\n",
    "            layers_dim=[input_dim, 32,16, 1],\n",
    "            num_bases=num_bases,\n",
    "            rank_factor=rank_factor,\n",
    "            ortho_mode='cayley',      # Ou 'hard', 'cayley', None\n",
    "            use_final_linear=False,  # Pour la couche de sortie (classification)\n",
    "            use_layernorm=True,     # Pour la stabilit√©\n",
    "            use_residual=False       # Pour la profondeur\n",
    "        )\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        batch_size = x_num.size(0)\n",
    "\n",
    "        # A. Vectorisation\n",
    "        num_vecs = self.num_proj(x_num).view(batch_size, -1)\n",
    "        cat_vecs = self.emb(x_cat).view(batch_size, -1)\n",
    "        x = torch.cat([num_vecs, cat_vecs], dim=1)\n",
    "\n",
    "        # B. Passage dans ton r√©seau\n",
    "        logits = self.backbone(x)\n",
    "\n",
    "        return logits.squeeze(-1)\n",
    "\n",
    "# --- Instanciation ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36e6b595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü•ä D√©marrage Entra√Ænement : Deep Low-Rank DSN\n",
      "--------------------------------------------------\n",
      "Epoch 1 | Loss: 0.5124 | VAL AUC: 0.7524 | Grad Norm: 0.3497 | Time: 9.7s\n",
      "Epoch 2 | Loss: 0.4802 | VAL AUC: 0.7620 | Grad Norm: 0.2645 | Time: 9.4s\n",
      "Epoch 3 | Loss: 0.4707 | VAL AUC: 0.7671 | Grad Norm: 0.2783 | Time: 9.4s\n",
      "Epoch 4 | Loss: 0.4626 | VAL AUC: 0.7685 | Grad Norm: 0.2820 | Time: 9.4s\n",
      "Epoch 5 | Loss: 0.4552 | VAL AUC: 0.7676 | Grad Norm: 0.2998 | Time: 9.2s\n",
      "Epoch 6 | Loss: 0.4472 | VAL AUC: 0.7652 | Grad Norm: 0.3180 | Time: 9.6s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m CriteoDeepWrapper(\n\u001b[1;32m      2\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20000\u001b[39m,\n\u001b[1;32m      3\u001b[0m     embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m      4\u001b[0m     num_bases\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m      5\u001b[0m     rank_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m \u001b[38;5;66;03m# Rank = Dim * 0.5\u001b[39;00m\n\u001b[1;32m      6\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Tu peux lancer le training direct.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Rappel MLP Score √† battre : ~0.7763\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrain_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDeep Low-Rank DSN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# --- 3. RELANCE LE DUEL ---\u001b[39;00m\n\u001b[1;32m     14\u001b[0m VOCAB_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20000\u001b[39m \n",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m, in \u001b[0;36mtrain_eval\u001b[0;34m(model, train_loader, val_loader, name)\u001b[0m\n\u001b[1;32m     23\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(x_n, x_c)\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, y)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# --- CALCUL DE LA NORME DU GRADIENT ---\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# On calcule la norme L2 de tous les gradients concat√©n√©s\u001b[39;00m\n\u001b[1;32m     29\u001b[0m batch_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model = CriteoDeepWrapper(\n",
    "    vocab_size=20000,\n",
    "    embed_dim=16,\n",
    "    num_bases=2,\n",
    "    rank_factor=0.1 # Rank = Dim * 0.5\n",
    ").to(device)\n",
    "# Tu peux lancer le training direct.\n",
    "\n",
    "# Rappel MLP Score √† battre : ~0.7763\n",
    "train_eval(model, train_loader, val_loader, name=\"Deep Low-Rank DSN\")\n",
    "\n",
    "# --- 3. RELANCE LE DUEL ---\n",
    "\n",
    "VOCAB_SIZE = 20000 \n",
    "# On garde les m√™mes hyperparam√®tres\n",
    "model_spectral = CriteoSpectralCross(vocab_size=VOCAB_SIZE, embed_dim=16).to(device)\n",
    "mlp_baseline = CriteoMLP(vocab_size=VOCAB_SIZE, embed_dim=16).to(device) # Ta classe MLP pr√©c√©dente\n",
    "\n",
    "print(\"‚ö° Lancement MLP (Rappel)...\")\n",
    "train_eval(mlp_baseline, train_loader, val_loader, name=\"Baseline MLP\")\n",
    "\n",
    "print(\"\\n‚ö° Lancement SPECTRAL CROSS (Optimis√©)...\")\n",
    "train_eval(model_spectral,train_loader, val_loader,  name=\"Spectral Cross DSN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee27b770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from SBN2 import DeepMultiBasisBilinearNet\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Assure-toi d'utiliser le mod√®le 32-16 (le bon compromis)\n",
    "model = CriteoDeepWrapper(vocab_size=20000, embed_dim=16, num_bases=4, rank_factor=0.25).to(device)\n",
    "\n",
    "def train_eval(model, train_loader, val_loader, name=\"Cosine-Stabilized\"):\n",
    "    # 1. WEIGHT DECAY : On passe √† 2e-3. C'est la \"gravit√©\" qui emp√™che la norme de monter.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)#, weight_decay=2e-5)\n",
    "    \n",
    "    # 2. SCHEDULER COSINE : Il freine en continu. Pas de surprise.\n",
    "    # T_max=10 correspond √† ton nombre d'√©poques.\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    print(f\"\\nü•ä D√©marrage Entra√Ænement : {name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for epoch in range(10): \n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_grad_norm = 0.0\n",
    "        \n",
    "        for x_n, x_c, y in train_loader:\n",
    "            x_n, x_c, y = x_n.to(device), x_c.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x_n, x_c)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # 3. CLIPPING : Toujours √† 0.20. C'est notre garde-fou.\n",
    "            norm_tensor = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.20)\n",
    "            total_grad_norm += norm_tensor.item()\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Le Scheduler avance √† chaque √©poque (ind√©pendamment de l'AUC)\n",
    "        scheduler.step()\n",
    "            \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_grad_norm = total_grad_norm / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        all_preds, all_y = [], []\n",
    "        with torch.no_grad():\n",
    "            for x_n, x_c, y in val_loader:\n",
    "                x_n, x_c, y = x_n.to(device), x_c.to(device), y.to(device)\n",
    "                logits = model(x_n, x_c)\n",
    "                probs = torch.sigmoid(logits)\n",
    "                all_preds.append(probs.cpu().numpy())\n",
    "                all_y.append(y.cpu().numpy())\n",
    "                \n",
    "        auc = roc_auc_score(np.concatenate(all_y), np.concatenate(all_preds))\n",
    "        dt = time.time() - t0\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | VAL AUC: {auc:.4f} | Norm: {avg_grad_norm:.4f} | LR: {current_lr:.1e} | Time: {dt:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "141f1d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from SBN2 import DeepMultiBasisBilinearNet\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "class CriteoDeepWrapper(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=16, num_bases=4, rank_factor=0.25):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # 1. Normalisation d√®s l'entr√©e num√©rique (Batch Norm 1D sur les 13 features)\n",
    "        self.input_norm = nn.BatchNorm1d(13)\n",
    "        self.num_proj = nn.Linear(13, 13 * embed_dim)\n",
    "        \n",
    "        input_dim = (13 + 26) * embed_dim \n",
    "\n",
    "        self.backbone = DeepMultiBasisBilinearNet(\n",
    "            layers_dim=[input_dim,32, 1],\n",
    "            num_bases=num_bases,\n",
    "            rank_factor=rank_factor,\n",
    "            ortho_mode='cayley',   # On garde Cayley, c'est tr√®s bien pour W\n",
    "            use_final_linear=False,\n",
    "            use_layernorm=True,\n",
    "            use_residual=False       \n",
    "        )\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        # 2. Pr√©-traitement Robuste : Log + BatchNorm\n",
    "        # On applique log(1+x) pour √©craser les ordres de grandeur (1000 -> 6.9)\n",
    "        x_num = torch.log1p(x_num.clamp(min=0)) \n",
    "        x_num = self.input_norm(x_num)\n",
    "        \n",
    "        batch_size = x_num.size(0)\n",
    "        num_vecs = self.num_proj(x_num).view(batch_size, -1)\n",
    "        cat_vecs = self.emb(x_cat).view(batch_size, -1)\n",
    "        x = torch.cat([num_vecs, cat_vecs], dim=1)\n",
    "        \n",
    "        logits = self.backbone(x)\n",
    "        return logits.squeeze(-1)\n",
    "\n",
    "# ... Instanciation ...\n",
    "model = CriteoDeepWrapper(vocab_size=20000, embed_dim=16, num_bases=4, rank_factor=0.25).to(device)\n",
    "\n",
    "def train_eval(model, train_loader, val_loader, name=\"Robust-Input-DSN\"):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)#, weight_decay=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    print(f\"\\nü•ä D√©marrage Entra√Ænement : {name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for epoch in range(10): \n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_grad_norm = 0.0 # Juste pour l'affichage\n",
    "        \n",
    "        for x_n, x_c, y in train_loader:\n",
    "            x_n, x_c, y = x_n.to(device), x_c.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x_n, x_c)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # --- CHANGEMENT CRUCIAL ICI ---\n",
    "            # 3. On utilise clip_grad_VALUE_ au lieu de Norm.\n",
    "            # Cela coupe les t√™tes qui d√©passent sans √©craser les petits gradients.\n",
    "            torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=0.1)\n",
    "            \n",
    "            # (Optionnel) On calcule quand m√™me la norme pour voir si elle baisse\n",
    "            # Mais on ne l'utilise plus pour clipper.\n",
    "            with torch.no_grad():\n",
    "                batch_norm = 0.0\n",
    "                for p in model.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        batch_norm += p.grad.norm(2).item() ** 2\n",
    "                batch_norm = batch_norm ** 0.5\n",
    "                total_grad_norm += batch_norm\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_grad_norm = total_grad_norm / len(train_loader)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # Validation...\n",
    "        model.eval()\n",
    "        all_preds, all_y = [], []\n",
    "        with torch.no_grad():\n",
    "            for x_n, x_c, y in val_loader:\n",
    "                # IMPORTANT : Appliquer la m√™me transfo log en validation !\n",
    "                # (Le mod√®le le fait dans le forward, donc c'est bon ici)\n",
    "                x_n, x_c, y = x_n.to(device), x_c.to(device), y.to(device)\n",
    "                logits = model(x_n, x_c)\n",
    "                all_preds.append(torch.sigmoid(logits).cpu().numpy())\n",
    "                all_y.append(y.cpu().numpy())\n",
    "                \n",
    "        auc = roc_auc_score(np.concatenate(all_y), np.concatenate(all_preds))\n",
    "        dt = time.time() - t0\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | VAL AUC: {auc:.4f} | Norm: {avg_grad_norm:.4f} | LR: {current_lr:.1e} | Time: {dt:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3448c41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü•ä D√©marrage Entra√Ænement : Deep Low-Rank DSN\n",
      "--------------------------------------------------\n",
      "Epoch 1 | Loss: 0.4977 | VAL AUC: 0.7566 | Norm: 0.2904 | LR: 2.0e-04 | Time: 57.2s\n",
      "Epoch 2 | Loss: 0.4715 | VAL AUC: 0.7618 | Norm: 0.2771 | LR: 1.8e-04 | Time: 53.8s\n",
      "Epoch 3 | Loss: 0.4556 | VAL AUC: 0.7596 | Norm: 0.3379 | LR: 1.6e-04 | Time: 53.9s\n",
      "Epoch 4 | Loss: 0.4359 | VAL AUC: 0.7535 | Norm: 0.4248 | LR: 1.3e-04 | Time: 54.9s\n",
      "Epoch 5 | Loss: 0.4127 | VAL AUC: 0.7432 | Norm: 0.5210 | LR: 1.0e-04 | Time: 192.4s\n",
      "Epoch 6 | Loss: 0.3891 | VAL AUC: 0.7365 | Norm: 0.6103 | LR: 6.9e-05 | Time: 179.6s\n",
      "Epoch 7 | Loss: 0.3681 | VAL AUC: 0.7272 | Norm: 0.6861 | LR: 4.1e-05 | Time: 168.4s\n",
      "Epoch 8 | Loss: 0.3511 | VAL AUC: 0.7236 | Norm: 0.7429 | LR: 1.9e-05 | Time: 67.6s\n",
      "Epoch 9 | Loss: 0.3392 | VAL AUC: 0.7201 | Norm: 0.7761 | LR: 4.9e-06 | Time: 68.0s\n",
      "Epoch 10 | Loss: 0.3327 | VAL AUC: 0.7190 | Norm: 0.7866 | LR: 0.0e+00 | Time: 1151.9s\n"
     ]
    }
   ],
   "source": [
    "# Rappel MLP Score √† battre : ~0.7763\n",
    "train_eval(model, train_loader, val_loader, name=\"Deep Low-Rank DSN\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
